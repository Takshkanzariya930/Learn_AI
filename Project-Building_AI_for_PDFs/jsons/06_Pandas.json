[{"number": "06", "title": "Pandas", "page_no": 1, "text": "Getting Started with Pandas\nWhat is Pandas?\nPandas is a powerful, open-source Python library used for data manipulation, \ncleaning, and analysis. It provides two main data structures:\nSeries: A one-dimensional labeled array\nDataFrame: A two-dimensional labeled table (like an Excel sheet or SQL table)\nPandas makes working with structured data fast, expressive, and flexible.\nIf you\u2019re working with tables, spreadsheets, or CSVs in Python\u2014Pandas is\nyour best friend.\nWhy Use Pandas?\nTask\nWithout Pandas\nWith Pandas\nLoad a CSV\nopen()  + loops\npd.read_csv()\nFilter rows\nCustom loop logic\ndf[df[\"col\"] > 5]\nGroup & summarize\nManual aggregation\ndf.groupby()\nMerge two datasets\nNested loops\npd.merge()\nPandas saves time, reduces code, and increases readability.\nInstalling Pandas\nInstall via pip:\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 2, "text": "Or using conda (recommended if you\u2019re using Anaconda):\nImporting Pandas\npd  is the standard alias used by the data science community.\nPandas vs Excel vs SQL vs NumPy\nTool\nStrengths\nWeaknesses\nExcel\nEasy UI, great for small data\nSlow, manual, not scalable\nSQL\nEfficient querying of big\ndata\nNot ideal for transformation\nlogic\nNumPy\nFast, low-level array\noperations\nNo labels, harder for tabular\ndata\nPandas\nLabel-aware, fast, flexible\nSlightly steep learning curve\nPandas bridges the gap between NumPy performance and Excel-like usability.\nPandas is built on top of NumPy.\nSummary\nUse Pandas when working with structured data.\npip install pandas\nconda install pandas\nimport pandas as pd\n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 3, "text": "It\u2019s the Swiss Army knife of data science.\nCore Data Structures in Pandas\nPandas is built on two main data structures:\nSeries \u2192 One-dimensional (like a single column in Excel)\nDataFrame \u2192 Two-dimensional (like a full spreadsheet or SQL table)\nSeries \u2014 1D Labeled Array\nA Series  is like a list with labels (index).\nOutput:\nNotice the automatic index: 0, 1, 2, 3\nYou can also define a custom index:\n\u2022 \n1. \n2. \nimport pandas as pd\ns = pd.Series([10, 20, 30, 40])\nprint(s)\n0\n10\n1\n20\n2\n30\n3\n40\ndtype: int64\ns = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\n"}, {"number": "06", "title": "Pandas", "page_no": 4, "text": "A pandas.Series  may look similar to a Python dictionary because both store data\nwith labels, but a Series offers much more. Unlike a dictionary, a Series supports\nfast vectorized operations, automatic index alignment during arithmetic, and\nhandles missing data using NaN . It also allows both label-based and position-\nbased access, and integrates seamlessly with the pandas ecosystem, especially\nDataFrames. While a dictionary is great for simple key\u2013value storage, a Series is\nbetter suited for data analysis and manipulation tasks where performance,\nflexibility, and built-in functionality matter.\nDataFrame \u2014 2D Labeled Table\nA DataFrame  is like a dictionary of Series \u2014 multiple columns with labels.\nOutput:\nEach column in a DataFrame  is a Series .\ndata = {\n\"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n\"age\": [25, 30, 35],\n\"city\": [\"Delhi\", \"Mumbai\", \"Bangalore\"]\n}\ndf = pd.DataFrame(data)\nprint(df)\n     name  age      city\n0   Alice   25     Delhi\n1     Bob   30    Mumbai\n2  Charlie   35  Bangalore\n"}, {"number": "06", "title": "Pandas", "page_no": 5, "text": "Index and Labels\nEvery Series and DataFrame has an Index \u2014 it helps with:\nFast lookups\nAligning data\nMerging & joining\nTime series operations\nYou can change them using:\nWhy Learn These Well?\nMost Pandas operations are built on these foundations:\nSelection\nFiltering\nMerging\nAggregation\nUnderstanding Series & DataFrames will make everything else easier.\nSummary\nSeries  = 1D array with labels\nDataFrame  = 2D table with rows + columns\n\u2022 \n\u2022 \n\u2022 \n\u2022 \ndf.index         # Row labels\ndf.columns       # Column labels\ndf.index = [\"a\", \"b\", \"c\"]\ndf.columns = [\"Name\", \"Age\", \"City\"]\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 6, "text": "Both come with index and are the heart of Pandas\nCreating DataFrames\nLet\u2019s look at different ways to create a Pandas DataFrame  \u2014 the core data\nstructure you\u2019ll be using 90% of the time in data science.\nFrom Python Lists\nFrom Dictionary of Lists\nMost common and readable format:\n\u2022 \nimport pandas as pd\ndata = [\n[\"Alice\", 25],\n[\"Bob\", 30],\n[\"Charlie\", 35]\n]\ndf = pd.DataFrame(data, columns=[\"Name\", \"Age\"])\nprint(df)\ndata = {\n\"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n\"Age\": [25, 30, 35]\n}\ndf = pd.DataFrame(data)\n"}, {"number": "06", "title": "Pandas", "page_no": 7, "text": "Each key becomes a column, and each list is the column data.\nFrom NumPy Arrays\nMake sure to provide column names!\nFrom CSV Files\nUse options like: - sep , header , names , index_col , usecols , nrows , etc.\nExample:\nFrom Excel Files\nYou may need to install openpyxl  or xlrd :\nimport numpy as np\narr = np.array([[1, 2], [3, 4]])\ndf = pd.DataFrame(arr, columns=[\"A\", \"B\"])\ndf = pd.read_csv(\"data.csv\")\npd.read_csv(\"data.csv\", usecols=[\"Name\", \"Age\"])\ndf = pd.read_excel(\"data.xlsx\")\npip install openpyxl\n"}, {"number": "06", "title": "Pandas", "page_no": 8, "text": "From JSON\nCan also read from a URL or string.\nFrom SQL Databases\nFrom the Web (Example: CSV from URL)\nEDA (Exploratory Data Analysis)\nExploratory Data Analysis (EDA) is an essential first step in any data science project.\nIt involves taking a deep look at the dataset to understand its structure, spot\npatterns, identify anomalies, and uncover relationships between variables. This\nprocess includes generating summary statistics, checking for missing or duplicate\ndata, and creating visualizations like histograms, box plots, and scatter plots. The\ngoal of EDA is to get a clear picture of what the data is telling you before applying\nany analysis or machine learning models.\ndf = pd.read_json(\"data.json\")\nimport sqlite3\nconn = sqlite3.connect(\"mydb.sqlite\")\ndf = pd.read_sql(\"SELECT * FROM users\", conn)\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\"\ndf = pd.read_csv(url)\n"}, {"number": "06", "title": "Pandas", "page_no": 9, "text": "By exploring the data thoroughly, you can make better decisions about how to\nclean, transform, and model it effectively.\nOnce your DataFrame is ready, run these to understand your data:\nSummary\nYou can create DataFrames from lists, dicts, arrays, files, web, and SQL\nUse .head() , .info() , .describe()  to quickly explore any dataset\nData Selection & Filtering\nSelecting the right rows and columns is the first step in analyzing any dataset.\nPandas gives you several powerful ways to do this.\nSelecting Rows & Columns\nSelecting Columns\ndf.head()\n# First 5 rows\ndf.tail()\n# Last 5 rows\ndf.info()\n# Column info: types, non-nulls\ndf.describe()\n# Stats for numeric columns\ndf.columns        # List of column names\ndf.shape          # (rows, columns)\n\u2022 \n\u2022 \ndf[\"column_name\"]\n# Single column (as Series)\ndf[[\"col1\", \"col2\"]]\n# Multiple columns (as DataFrame)\n"}, {"number": "06", "title": "Pandas", "page_no": 10, "text": "Selecting Rows by Index\nUse .loc[]  (label-based) and .iloc[]  (position-based):\nSelect Specific Rows and Columns\nYou can also slice:\nFast Access: .at  and .iat\nThese are optimized for single element access:\nFiltering with Conditions\nSimple Condition\ndf.loc[0]\n# First row (by label)\ndf.iloc[0]\n# First row (by position)\ndf.loc[0, \"Name\"]\n# Value at row 0, column 'Name'\ndf.iloc[0, 1]\n# Value at row 0, column at index 1\ndf.loc[0:2, [\"Name\", \"Age\"]]\n# Rows 0 to 2, selected columns\ndf.iloc[0:2, 0:2]\n# Rows and cols by index position\ndf.at[0, \"Name\"]\n# Fast label-based access\ndf.iat[0, 1]\n# Fast position-based access\ndf[df[\"Age\"] > 30]\n"}, {"number": "06", "title": "Pandas", "page_no": 11, "text": "Multiple Conditions (AND / OR)\nUse parentheses around each condition!\nQuerying with .query()\nThe .query()  method in pandas lets you filter DataFrame rows using a string\nexpression \u2014 it\u2019s a more readable and often more concise alternative to using\nboolean indexing.\nThis is a cleaner, SQL-like way to filter:\nDynamic column names:\nHere are the main rules and tips for using .query()  in pandas:\n1. Column names become variables\nYou can reference column names directly in the query string:\ndf[(df[\"Age\"] > 25) & (df[\"City\"] == \"Delhi\")]\ndf[(df[\"Name\"] == \"Bob\") | (df[\"Age\"] < 30)]\ndf.query(\"Age > 25 and City == 'Delhi'\")\ncol = \"Age\"\ndf.query(f\"{col} > 25\")\ndf.query(\"age > 25 and city == 'Delhi'\")\n"}, {"number": "06", "title": "Pandas", "page_no": 12, "text": "2. String values must be in quotes\nUse single or double quotes around strings in the expression:\nIf you have quotes inside quotes, mix them:\n3. Use backticks for column names with spaces or special\ncharacters\nIf a column name has spaces, use backticks ( ` ):\n4. You can use @  to reference Python variables\nTo pass external variables into .query() :\n5. Logical operators\nUse these: - and , or , not  \u2014 instead of & , | , ~  - == , != , < , > , <= , >=\nBad:\ndf.query(\"name == 'Harry'\")\ndf.query('city == \"Mumbai\"')\ndf.query(\"`first name` == 'Alice'\")\nage_limit = 30\ndf.query(\"age > @age_limit\")\n"}, {"number": "06", "title": "Pandas", "page_no": 13, "text": "Good:\n6. Chained comparisons\nJust like Python:\n7. Avoid using reserved keywords as column names\nIf you have a column named class , lambda , etc., you\u2019ll need to use backticks:\n8. Case-sensitive\nColumn names and string values are case-sensitive:\n9. .query()  returns a copy, not a view\nThe result is a new DataFrame. Changes won\u2019t affect the original unless reassigned:\ndf.query(\"age > 30 & city == 'Delhi'\")\n# \u274c\ndf.query(\"age > 30 and city == 'Delhi'\")\n# \u2705\ndf.query(\"25 < age <= 40\")\ndf.query(\"`class` == 'Physics'\")\ndf.query(\"City == 'delhi'\")\n# \u274c if actual value is 'Delhi'\n"}, {"number": "06", "title": "Pandas", "page_no": 14, "text": "Summary\nUse df[col] , .loc[] , .iloc[] , .at[] , .iat[]  to access data\nFilter with logical conditions or .query()  for readable code\nMastering selection makes the rest of pandas feel easy\nData Cleaning & Preprocessing\nReal-world data is messy. Pandas gives us powerful tools to clean and transform\ndata before analysis.\nHandling Missing Values\nCheck for Missing Data\nDrop Missing Data\nfiltered = df.query(\"age < 50\")\n\u2022 \n\u2022 \n\u2022 \ndf.isnull()\n# True for NaNs\ndf.isnull().sum()\n# Count missing per column\ndf.dropna()\n# Drop rows with *any* missing values\ndf.dropna(axis=1)\n# Drop columns with missing values\n"}, {"number": "06", "title": "Pandas", "page_no": 15, "text": "Fill Missing Data\nIn pandas, fillna is used to fill unknown values. ffill and bfill are methods used to fill\nmissing values (like NaN, None, or pd.NA) by propagating values forward or\nbackward.\nDetecting & Removing Duplicates\ndf.duplicated() returns a boolean Series where: True means that row is a duplicate\nof a previous row. False means it\u2019s the first occurrence (not a duplicate yet).\nCheck based on specific columns:\nString Operations with .str\nWorks like vectorized string methods and returns a pandas Series:\ndf.fillna(0)\n# Replace NaN with 0\ndf[\"Age\"].fillna(df[\"Age\"].mean())\n# Replace with mean\ndf.ffill()\n# Forward fill\ndf.bfill()\n# Backward fill\ndf.duplicated()\n# True for duplicates\ndf.drop_duplicates()\n# Remove duplicate rows\ndf.duplicated(subset=[\"Name\", \"Age\"])\ndf[\"Name\"].str.lower() # Converts all names to lowercase.\ndf[\"City\"].str.contains(\"delhi\", case=False) # Checks if 'delhi' is in the city \nname, case-insensitive.\ndf[\"Email\"].str.split(\"@\") # Outputs a pandas Series where each element is a list \n"}, {"number": "06", "title": "Pandas", "page_no": 16, "text": "We can always chain methods like .str.strip().str.upper()  for clean-up.\nType Conversions with .astype()\nConvert column data types:\nWhy is pd.to_datetime() special?\nUnlike astype(), which works on simple data types (like integers, strings, etc.),\npd.to_datetime() is designed to:\nHandle different date formats (e.g., \u201cYYYY-MM-DD\u201d, \u201cMM/DD/YYYY\u201d, etc.).\nHandle mixed types (e.g., some date strings, some NaT, or missing values).\nConvert integer timestamps (e.g., UNIX time) into datetime objects.\nRecognize timezones if provided.\nCheck data types:\nof strings (the split parts). This is where a Python list comes into play, but the \nouter object is still a pandas Series.\ndf[\"Age\"] = df[\"Age\"].astype(int)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Category\"] = df[\"Category\"].astype(\"category\")\n\u2022 \n\u2022 \n\u2022 \n\u2022 \ndf.dtypes\n"}, {"number": "06", "title": "Pandas", "page_no": 17, "text": "Applying Functions\n.apply()  \u2192 Apply any function to rows or columns\n.map()  \u2192 Element-wise mapping for Series\n.replace()  \u2192 Replace specific values\nSummary\nUse isnull() , fillna() , dropna()  for missing data\nClean text with .str , convert types with .astype()\nUse apply() , map() , replace()  to transform your columns\nData cleaning is where 80% of your time goes in real projects\nData Transformation\nOnce your data is clean, the next step is to reshape, reformat, and reorder it as\nneeded for analysis. Pandas gives you plenty of flexible tools to do this.\ndf[\"Age Group\"] = df[\"Age\"].apply(lambda x: \"Adult\" if x >= 18 else \"Minor\")\ngender_map = {\"M\": \"Male\", \"F\": \"Female\"}\ndf[\"Gender\"] = df[\"Gender\"].map(gender_map)\ndf[\"City\"].replace({\"Del\": \"Delhi\", \"Mum\": \"Mumbai\"})\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 18, "text": "Sorting & Ranking\nSort by Values\ndf.sort_values([\u201cAge\u201d, \u201cSalary\u201d]) sorts the DataFrame first by the \u201cAge\u201d column, and\nif there are ties (i.e., two or more rows with the same \u201cAge\u201d), it will sort by the\n\u201cSalary\u201d column.\nReset Index\nIf you want the index to start from 0 and be sequential, you can reset it using\nreset_index()\nSort by Index\nThe df.sort_index() function is used to sort the DataFrame based on its index\nvalues. If the index is not in a sequential order (e.g., you have dropped rows or\nperformed other operations that change the index), you can use sort_index() to\nrestore it to a sorted order. ### Ranking The .rank() function in pandas is used to\nassign ranks to numeric values in a column, like scores or points. By default, it gives\nthe average rank to tied values, which can result in decimal numbers. For example,\nif two people share the top score, they both get a rank of 1.5. You can customize\nthe ranking behavior using the method parameter. One useful option is\nmethod=\u2018dense\u2019, which assigns the same rank to ties but doesn\u2019t leave gaps in the\nranking sequence. This is helpful when you want a clean, consecutive ranking\nsystem without skips.\ndf.sort_values(\"Age\")\n# Ascending sort\ndf.sort_values(\"Age\", ascending=False)\n# Descending\ndf.sort_values([\"Age\", \"Salary\"])\n# Sort by multiple columns\ndf.reset_index(drop=True, inplace=True)\n# Reset the index and drop the old index\ndf.sort_index()\n"}, {"number": "06", "title": "Pandas", "page_no": 19, "text": "Renaming Columns & Index\nTo rename all columns:\nChanging Column Order\nJust pass a new list of column names:\nYou can also move one column to the front:\nSummary\nSort, rank, and rename to prepare your data\nReordering and reshaping are key for EDA and visualization\ndf[\"Rank\"] = df[\"Score\"].rank()\n# Default: average method\ndf[\"Rank\"] = df[\"Score\"].rank(method=\"dense\")\n# 1, 2, 2, 3\ndf.rename(columns={\"oldName\": \"newName\"}, inplace=True)\ndf.rename(index={0: \"row1\", 1: \"row2\"}, inplace=True)\ndf.columns = [\"Name\", \"Age\", \"City\"]\ndf = df[[\"City\", \"Name\", \"Age\"]]\n# Reorder as desired\ncols = [\"Name\"] + [col for col in df.columns if col != \"Name\"]\ndf = df[cols]\n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 20, "text": "Reshaping Data using Melt and Pivot\nmelt()  \u2014 Wide to Long\nThe melt()  method in Pandas is used to unpivot a DataFrame from wide format\nto long format. In other words, it takes columns that represent different variables\nand combines them into key-value pairs (i.e., long-form data).\nWhen to Use melt() :\nWhen you have a DataFrame where each row is an observation, and each\ncolumn represents a different variable or measurement, and you want to\nreshape the data into a longer format for easier analysis or visualization.\nSyntax:\nParameters:\nid_vars : The columns that you want to keep fixed (these columns will remain\nas identifiers).\nvalue_vars : The columns you want to unpivot (the ones you want to \u201cmelt\u201d\ninto a single column).\nvar_name : The name to use for the new column that will contain the names of\nthe melted columns (default is 'variable' ).\nvalue_name : The name to use for the new column that will contain the values\nfrom the melted columns (default is 'value' ).\ncol_level : Used for multi-level column DataFrames.\nExample:\nUse this code to generate the Dataframe\n\u2022 \ndf.melt(id_vars=None, value_vars=None, var_name=None, value_name=\"value\",\ncol_level=None)\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 21, "text": "Let\u2019s say we have the following DataFrame in a wide format:\nName\nMath\nScience\nEnglish\nAlice\n85\n90\n88\nBob\n78\n82\n85\nCharlie\n92\n89\n94\nUsing melt() :\nIf we want to \u201cmelt\u201d the DataFrame so that each row represents a student-subject\npair, we can do:\nThis will result in the following long-format DataFrame:\nName\nSubject\nScore\nAlice\nMath\n85\nimport pandas as pd\n# Sample DataFrame\ndata = {\n'Name': ['Alice', 'Bob', 'Charlie'],\n'Math': [85, 78, 92],\n'Science': [90, 82, 89],\n'English': [88, 85, 94]\n}\ndf = pd.DataFrame(data)\n# Display the DataFrame\nprint(df)\ndf.melt(id_vars=[\"Name\"], value_vars=[\"Math\", \"Science\", \"English\"], var_name=\"Subj\nect\", value_name=\"Score\")\n"}, {"number": "06", "title": "Pandas", "page_no": 22, "text": "Name\nSubject\nScore\nAlice\nScience\n90\nAlice\nEnglish\n88\nBob\nMath\n78\nBob\nScience\n82\nBob\nEnglish\n85\nCharlie\nMath\n92\nCharlie\nScience\n89\nCharlie\nEnglish\n94\nExplanation:\nid_vars=[\"Name\"] : We keep the \u201cName\u201d column as it is because it\u2019s the\nidentifier.\nvalue_vars=[\"Math\", \"Science\", \"English\"] : These are the columns we\nwant to melt.\nvar_name=\"Subject\" : The new column containing the names of the subjects.\nvalue_name=\"Score\" : The new column containing the scores.\nWhy Use melt() ?\nData normalization: Helps in transforming data for statistical modeling and\ndata visualization.\nPivot tables: Many times, plotting functions or statistical models work better\nwith long-format data.\nThis is useful for converting columns into rows \u2014 perfect for plotting or tidy data\nformats.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 23, "text": "pivot()  \u2014 Long to Wide\nThe pivot()  function in Pandas is used to reshape data, specifically to turn long-\nformat data into wide-format data. This is the reverse operation of melt() .\nHow it works:\npivot()  takes a long-format DataFrame and turns it into a wide-format\nDataFrame by specifying which columns will become the new columns, the\nrows, and the values.\nSyntax:\nParameters:\nindex : The column whose unique values will become the rows of the new\nDataFrame.\ncolumns : The column whose unique values will become the columns of the\nnew DataFrame.\nvalues : The column whose values will fill the new DataFrame. These will\nbecome the actual data (values in the table).\nExample:\nSuppose we have the following long-format DataFrame:\nName\nSubject\nScore\nAlice\nMath\n85\nAlice\nScience\n90\nAlice\nEnglish\n88\nBob\nMath\n78\nBob\nScience\n82\n\u2022 \ndf.pivot(index=None, columns=None, values=None)\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "06", "title": "Pandas", "page_no": 24, "text": "Name\nSubject\nScore\nBob\nEnglish\n85\nCharlie\nMath\n92\nCharlie\nScience\n89\nCharlie\nEnglish\n94\nUsing pivot()  to reshape it into wide format:\nResulting DataFrame:\nName\nEnglish\nMath\nScience\nAlice\n88\n85\n90\nBob\n85\n78\n82\nCharlie\n94\n92\n89\nExplanation:\nindex=\"Name\" : The unique values in the \u201cName\u201d column will become the rows\nin the new DataFrame.\ncolumns=\"Subject\" : The unique values in the \u201cSubject\u201d column will become\nthe columns in the new DataFrame.\nvalues=\"Score\" : The values from the \u201cScore\u201d column will populate the table.\nWhy use pivot() ?\nBetter data structure: It makes data easier to analyze when you have\ncategories that you want to split into multiple columns.\nEasier visualization: Often, you want to represent data in a format where\ncategories are split across columns (for example, when creating pivot tables\nfor reporting).\ndf.pivot(index=\"Name\", columns=\"Subject\", values=\"Score\")\n\u2022 \n\u2022 \n\u2022 \n1. \n2. \n"}, {"number": "06", "title": "Pandas", "page_no": 25, "text": "Aggregating data: You can perform aggregations (like sum , mean , etc.) to\ngroup values before pivoting.\nImportant Notes:\nDuplicate Entries: If you have multiple rows with the same combination of \nindex  and columns , pivot() will raise an error. In such cases, you should use \npivot_table()  (which can handle duplicate entries by aggregating them).\nExample of pivot_table()  to handle duplicates:\nSuppose the DataFrame is like this (with duplicate entries):\nName\nSubject\nScore\nAlice\nMath\n85\nAlice\nMath\n80\nAlice\nScience\n90\nBob\nMath\n78\nBob\nMath\n82\nWe can use pivot_table()  to aggregate values (e.g., taking the mean for\nduplicate entries):\nResulting DataFrame:\nName\nMath\nScience\nAlice\n82.5\n90\nBob\n80\nNaN\nIn this case, the Math score for Alice is averaged (85 + 80) / 2 = 82.5. If a cell is\nempty, it means there was no value for that combination.\n3. \n1. \ndf.pivot_table(index=\"Name\", columns=\"Subject\", values=\"Score\", aggfunc=\"mean\")\n"}, {"number": "06", "title": "Pandas", "page_no": 26, "text": "Summary:\nUse melt()  to go long, pivot()  to go wide\npivot()  is used to turn long-format data into wide-format by spreading\nunique column values into separate columns.\nIf there are duplicate values for a given combination of index  and columns ,\nyou should use pivot_table()  with an aggregation function to handle the\nduplicates.\nAggregation & Grouping\nGrouping and aggregating helps you summarize your data \u2014 like answering:\n\u201cWhat\u2019s the average salary per department?\u201d\n\u201cHow many users joined the Gym per month?\u201d\n.groupby()  Function\ndf.groupby() is used to group rows of a DataFrame based on the values in one or\nmore columns, which allows you to then perform aggregate functions (like sum(),\nmean(), count(), etc.) on each group. Consider this DataFrame:\n\u2022 \n\u2022 \n\u2022 \ndf = pd.DataFrame({\n\"Department\": [\"HR\", \"HR\", \"IT\", \"IT\", \"Marketing\", \"Marketing\", \"Sales\", \"Sale\ns\"],\n\"Team\": [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\"],\n\"Gender\": [\"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\"],\n\"Salary\": [85, 90, 78, 85, 92, 88, 75, 80],\n\"Age\": [23, 25, 30, 22, 28, 26, 21, 27],\n\"JoinDate\": pd.to_datetime([\n\"2020-01-10\", \"2020-02-15\", \"2021-03-20\", \"2021-04-10\",\n\"2020-05-30\", \"2020-06-25\", \"2021-07-15\", \"2021-08-01\"\n])\n})\n"}, {"number": "06", "title": "Pandas", "page_no": 27, "text": "This says:\n> \u201cGroup by Department, then calculate average Salary for each group.\u201d\nCommon Aggregation Functions\nTo group by multiple columns:\nCustom Aggregations with .agg()\nApply multiple functions at once like this:\nIn pandas, .agg and .aggregate are exactly the same \u2014 they\u2019re aliases for the same\nmethod\nName your own functions:\ndf.groupby(\"Department\")[\"Salary\"].mean()\ndf.groupby(\"Team\")[\"Salary\"].mean()\n# Average per team\ndf.groupby(\"Team\")[\"Salary\"].sum()\n# Total score\ndf.groupby(\"Team\")[\"Salary\"].count()\n# How many entries\ndf.groupby(\"Team\")[\"Salary\"].min()\ndf.groupby(\"Team\")[\"Salary\"].max()\ndf.groupby([\"Team\", \"Gender\"])[\"Salary\"].mean()\ndf.groupby(\"Team\")[\"Salary\"].agg([\"mean\", \"max\", \"min\"])\ndf.groupby(\"Team\")[\"Salary\"].agg(\n    avg_score=\"mean\",\n"}, {"number": "06", "title": "Pandas", "page_no": 28, "text": "Apply different functions to different columns:\nTransform vs Aggregate vs Filter\nOperation\nReturns\nWhen to Use\n.aggregate()\nSingle value per\ngroup\nSummary (like mean)\n.transform()\nSame shape as\noriginal\nAdd new column based on\ngroup\n.filter()\nSubset of rows\nKeep/discard whole groups\n.transform()  Example:\nNow each row gets its team average \u2014 great for comparisons!\n.filter()  Example:\nOnly keeps teams with average score > 80.\n    high_score=\"max\"\n)\ndf.groupby(\"Team\").agg({\n\"Salary\": \"mean\",\n\"Age\": \"max\"\n})\ndf[\"Team Avg\"] = df.groupby(\"Team\")[\"Salary\"].transform(\"mean\")\ndf.groupby(\"Team\").filter(lambda x: x[\"Salary\"].mean() > 80)\n"}, {"number": "06", "title": "Pandas", "page_no": 29, "text": "Summary\n.groupby()  helps you summarize large datasets by category\nUse mean() , sum() , count() , .agg()  for custom metrics\n.transform()  adds values back to original rows\n.filter()  keeps only groups that meet conditions\n# Merging & Joining Data\nOften, data is split across multiple tables or files. Pandas lets you combine them\njust like SQL \u2014 or even more flexibly!\nSample DataFrames\nMerge Like SQL: pd.merge()\nInner Join (default)\nReturns only matching DeptIDs:\n\u2022 \n\u2022 \n\u2022 \n\u2022 \nemployees = pd.DataFrame({\n\"EmpID\": [1, 2, 3],\n\"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n\"DeptID\": [10, 20, 30]\n})\ndepartments = pd.DataFrame({\n\"DeptID\": [10, 20, 40],\n\"DeptName\": [\"HR\", \"Engineering\", \"Marketing\"]\n})\npd.merge(employees, departments, on=\"DeptID\")\n"}, {"number": "06", "title": "Pandas", "page_no": 30, "text": "EmpID\nName\nDeptID\nDeptName\n1\nAlice\n10\nHR\n2\nBob\n20\nEngineering\nLeft Join\nKeeps all employees, fills NaN  where no match.\nRight Join\nKeeps all departments, even if no employee.\nOuter Join\nIncludes all data, fills missing with NaN .\nConcatenating DataFrames\nUse pd.concat()  to stack datasets either vertically or horizontally.\npd.merge(employees, departments, on=\"DeptID\", how=\"left\")\npd.merge(employees, departments, on=\"DeptID\", how=\"right\")\npd.merge(employees, departments, on=\"DeptID\", how=\"outer\")\n"}, {"number": "06", "title": "Pandas", "page_no": 31, "text": "Vertical (rows)\nHorizontal (columns)\nMake sure indexes align when using axis=1\nWhen to Use What?\nUse Case\nMethod\nSQL-style joins (merge keys)\npd.merge()\nor .join()\nStack datasets vertically\npd.concat([df1, \ndf2])\nCombine different features side-by-side\npd.concat([df1, \ndf2], axis=1)\nAlign on index\n.join()  or\nmerge with \nright_index=Tru\ne\ndf1 = pd.DataFrame({\"Name\": [\"Alice\", \"Bob\"]})\ndf2 = pd.DataFrame({\"Name\": [\"Charlie\", \"David\"]})\npd.concat([df1, df2])\ndf1 = pd.DataFrame({\"ID\": [1, 2]})\ndf2 = pd.DataFrame({\"Score\": [90, 80]})\npd.concat([df1, df2], axis=1)\n"}, {"number": "06", "title": "Pandas", "page_no": 32, "text": "Summary\nUse merge()  like SQL joins ( inner , left , right , outer )\nUse concat()  to stack DataFrames (rows or columns)\nHandle mismatched keys and indexes with care\nMerging and joining are essential for real-world projects\nReading & Writing Files in Pandas\nCSV Files\nRead CSV\nOptions:\nWrite CSV\nExcel Files\nRead Excel\n\u2022 \n\u2022 \n\u2022 \n\u2022 \ndf = pd.read_csv(\"data.csv\")\npd.read_csv(\"data.csv\", usecols=[\"Name\", \"Age\"], nrows=10)\ndf.to_csv(\"output.csv\", index=False)\ndf = pd.read_excel(\"data.xlsx\")\n"}, {"number": "06", "title": "Pandas", "page_no": 33, "text": "Options:\nWrite Excel\nMultiple sheets:\nJSON Files\nRead JSON\nSummary\nread_*  and to_*  methods for CSV, Excel, JSON\nUse sheet_name  for Excel\npd.read_excel(\"data.xlsx\", sheet_name=\"Sales\")\ndf.to_excel(\"output.xlsx\", index=False)\nwith pd.ExcelWriter(\"report.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Summary\", index=False)\n    df2.to_excel(writer, sheet_name=\"Details\", index=False)\ndf = pd.read_json(\"data.json\")\n\u2022 \n\u2022 \n"}]