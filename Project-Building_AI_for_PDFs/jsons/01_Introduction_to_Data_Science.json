[{"number": "01", "title": "Introduction_to_Data_Science", "page_no": 1, "text": "Introduction to Data Science\nWhat is Data Science?\nWelcome to this course on Data Science. If you are active on Fb/Insta/X, share your\nData Science journey on these platforms and tag me. I would love to see your\nprogress.\nI am so happy you decided to learn Data Science with me in my style. Lets step\nback and talk a bit about why do we need data science. What kind of tasks we will\ndo as a data scientist in an organization\nWhat is Data Science?\nAt its core, Data Science is the field of study that uses mathematics, statistics,\nprogramming, and domain knowledge to extract meaningful insights from data.\nWell that sounds like a bookish definition which you are free to write in your\nsemester exams but Data Science pretty much blends various techniques from\ndifferent disciplines to analyze large amounts of information and solve real-world\nproblems.\nSimple Definition:\nLet me give you a very simple non fancy definition of Data Science\nData Science is the process of collecting, cleaning, analyzing, and interpreting\ndata to make informed decisions.\n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 2, "text": "Why is Data Science Important?\nIn today\u2019s world, data is everywhere\u2014from your online shopping habits to the\nsensors in smart devices. Companies use this data to:\nMake better decisions (e.g., which product to launch next).\nPredict outcomes (e.g., weather forecasts or stock prices).\nAutomate processes (e.g., self-driving cars).\nPersonalize experiences (e.g., Netflix recommendations, YouTube\nrecommendations).\nKey Steps in Data Science:\nData Collection \u2013 Gathering raw data from various sources (websites,\ndatabases, IoT devices, etc.).\nData Cleaning \u2013 Fixing errors and handling missing values (this takes up 80%\nof a data scientist\u2019s time).\nData Analysis \u2013 Using statistical methods to find patterns and insights.\nModel Building \u2013 Applying machine learning to make predictions.\nInterpretation & Communication \u2013 Presenting findings in a clear way to help\ndecision-making.\nWhere Do We See Data Science in Action?\nHealthcare: Predicting disease outbreaks and improving diagnoses.\nE-commerce: Personalized product recommendations.\nFinance: Detecting fraudulent transactions by recognizing patterns.\nEntertainment: Content recommendations on platforms like YouTube and\nNetflix.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n1. \n2. \n3. \n4. \n5. \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 3, "text": "Why Should You Care About Data Science?\nHigh Demand: There\u2019s a global shortage of skilled data scientists.\nGreat Pay: The average salary of a data scientist in the US is $120,000+ per\nyear; in India, \u20b910-30 LPA for experienced professionals.\nFuture-Proof Career: It powers everything from AI to business strategy\u2014this\nfield is only growing.\nNote: The content you just read will be supplied to you as a downloadable PDF.\nSince I am writing a summarized version of the video content which will serve as\nrevision notes, I recommend you try to read them along with watching the videos.\nThanks and see you in the next lecture!\nData Science Lifecycle\nThe Data Science Lifecycle refers to the structured process used to extract insights\nfrom data. It involves several stages, from gathering raw data to delivering\nactionable insights. Here is a breakdown of each step:\n1. Problem Definition\nUnderstanding the problem you want to solve.\nIdentify business objectives and define the question to answer.\nLater, we will do a very interesting project called \u201cCoders of Delhi\u201d where we\nstart by understanding the business objective.\nExample: \u201cCan we predict customer churn?\u201d or \u201cWhat factors drive sales?\u201d. In\n\u201cCoders of Delhi\u201d project, the problem is how to find potential friends of a\ngiven person in a social network\nKey Activities:\nCollaborate with stakeholders.\nDefine success metrics.\nSet project goals and deliverables.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 4, "text": "2. Data Collection\nGathering relevant data from multiple sources.\nSources may include databases, APIs, web scraping, or third-party datasets.\nSometimes if this step is taken care of by another team or it\u2019s a data dump\ngiven by another team, we don\u2019t care where it came from.\nKey Activities in data collection:\nIdentify data sources (structured vs.\u00a0unstructured).\nCollect data using SQL, Python, or automated pipelines.\nEnsure data relevance and completeness.\n3. Data Cleaning (Data Preprocessing)\nPreparing raw data for analysis.\nThis step addresses missing values, duplicates, and inconsistencies.\nKey Activities:\nHandle missing or incorrect data.\nStandardize formats and remove duplicates.\nManage outliers and inconsistencies.\nFun Fact: Data scientists spend 80% of their time cleaning data!\n4. Data Exploration (EDA \u2013 Exploratory Data Analysis)\nAnalyzing data patterns and relationships.\nUnderstand data distributions and detect anomalies using visualizations.\nKey Activities:\nSummarize data using statistics (mean, median, etc.).\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 5, "text": "Visualize patterns (using Matplotlib, Seaborn, etc.).\nIdentify correlations and outliers. (correlation is how two variables move in\nrelation to each other and outlier is a data point that stands out as unusually\ndifferent from the rest. Eg. A 190 Kg heavyweight person is an outlier)\n5. Model Building\nCreating and training machine learning models.\nUse algorithms to predict outcomes or classify data.\nKey Activities:\nChoose appropriate models (e.g., regression, decision trees, neural networks).\nSplit data into training and testing sets.\nTrain and fine-tune models.\nCommon Tools: Scikit-learn, TensorFlow, PyTorch.\n6. Model Evaluation\nMeasuring model performance and accuracy.\nEvaluate models using metrics to ensure reliability.\nKey Activities:\nUse performance metrics (e.g., accuracy, RMSE, ROC curve) to answer\nquestions like - \u201cHow often is my model correct?\u201d\nPerform cross-validation for robustness. Train using some part of the data and\ntest using some part and average out the accuracy. We will study this in detail\nlater\nCompare multiple models for best outcomes.\nKey Metrics:\nClassification: Accuracy, Precision, Recall, F1-Score.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 6, "text": "Regression: RMSE, R-squared.\n7. Deployment\nIntegrating the model into production systems.\nDeliver actionable results through APIs or dashboards.\nKey Activities:\nPackage the model for deployment (Usually done using web frameworks like\nFlask, and FastAPI).\nAutomate pipelines for continuous learning (MLOps).\nMonitor performance post-deployment.\n8. Communication & Reporting\nSharing insights with stakeholders. At the end of the day the ML model solves\na problem and proper reporting it to the concerned department is very\nimportant\nKey Activities:\nCreate dashboards\nPresent findings clearly and concisely.\nDocument the process and results.\n9. Maintenance & Iteration\nKeeping the model accurate and up-to-date.\nKey Activities:\nMonitor model performance.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 7, "text": "Update models with new data.\nRefine features and parameters.\nSummary\nThe Data Science Lifecycle is a continuous, iterative process involving:\nProblem Definition\nData Collection\nData Cleaning\nData Exploration\nModel Building\nModel Evaluation\nDeployment\nCommunication & Reporting\nMaintenance & Iteration\nBy following this lifecycle, data scientists transform raw data into meaningful\ninsights that drive better decision-making.\nData Science Tools\nI am enjoying teaching so far. When working in data science, the right tools make\nyour work easier, faster, and more efficient. When I started my data science journey\nat IIT Kharagpur, I used to code using Pycharm and regular Python installation. I\nknew about Jupyter but wasn\u2019t familiar with its capabilities. From writing code to\nvisualizing data, there are many options to choose from. Here is a breakdown of\npopular data science tools and why Anaconda with Jupyter Notebook is an\nexcellent choice for beginners and advanced users.\nHow to run Python programs\nThe easiest way to run Python programs is by installing VS Code and using pip to\ninstall packages but we will use Anaconda and Jupyter notebooks\n\u2022 \n\u2022 \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 8, "text": "1. Jupyter Notebook (with Anaconda Distribution)\nAn open-source web application that allows you to create and share\ndocuments with live code, equations, visualizations, and text.\nWhy Use Anaconda with Jupyter Notebook?\nUser-Friendly: Interactive coding and easy-to-follow outputs, perfect for\nbeginners.\nAll-in-One Package: Anaconda includes essential libraries (NumPy, Pandas,\nMatplotlib + 1,500 other popular packages) pre-installed.\nIdeal for Data Science: Quick prototyping, data visualization, and exploratory\nanalysis.\nEnvironment Management: Easily create isolated environments to manage\npackage dependencies.\nCommon Use Cases:\nData exploration and visualization\nMachine learning experiments\nSharing research and reports\nInstallation: Althought we will do a detailed installation of Anaconda in the later\nsections, you can download and install the Anaconda distribution from \nanaconda.com. It includes Jupyter Notebook by default.\nCommand to Launch Jupyter Notebook:\nDon\u2019t worry, we will do all these things step by step in the next section\n2. Google Colab\nA free, cloud-based Jupyter Notebook environment provided by Google.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \njupyter notebook\n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 9, "text": "Why Use Google Colab?\nFree GPU/TPU Access: Great for deep learning without requiring expensive\nhardware.\nCloud-Based: No local setup\u2014just log in and start coding.\nCollaboration: Share notebooks via links for easy collaboration.\nCommon Use Cases:\nMachine learning and deep learning projects\nQuick experiments without local setup\nCollaborative projects\nAccess: Use Google Colab directly in your browser at colab.research.google.com.\n3. VS Code (Visual Studio Code)\nA lightweight and powerful code editor by Microsoft with robust extensions.\nWhy Use VS Code?\nCustomizable: Extensive extensions for Python and data science (e.g., Python\nextension by Microsoft).\nIntegrated Jupyter Support: You can run Jupyter Notebooks directly in VS\nCode.\nDebugging Tools: Advanced debugging capabilities.\nCommon Use Cases:\nLarge-scale data science projects\nWorking with multiple languages (Python, R, etc.)\nIntegrated development (data pipelines, APIs)\nInstallation: Can be downloaded from code.visualstudio.com but we will install and\nuse Anaconda distribution throughout this Data Science course.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 10, "text": "4. PyCharm\nA powerful, professional IDE for Python development by JetBrains.\nWhy Use PyCharm?\nProfessional Features: Advanced code navigation, refactoring, and debugging.\nEnvironment Management: Virtual environment and package management\nbuilt-in.\nScientific Mode: Built-in support for Jupyter notebooks.\nCommon Use Cases:\nLarge, production-level data science projects\nBuilding Python-based machine learning applications\nInstallation: Download from jetbrains.com/pycharm.\n5. Cursor AI\nAn AI-powered code editor designed for enhanced productivity with machine\nlearning assistance.\nWhy Use Cursor AI?\nAI Integration: Code suggestions and completions for faster development.\nContext-Aware: Understands complex data science workflows.\nCollaborative: Works well with team-based projects.\nCommon Use Cases:\nAssisted coding for data science\nAccelerating research and prototyping\nTeam collaboration\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 11, "text": "Access: You can visit cursor.so to download cursor AI but I don\u2019t recommend using\nit just yet. Its important to understand the basics of data science and programming\nbefore you use such AI assistants\nWhich Tool Should You Choose?\nTool\nBest For\nKey Advantage\nJupyter\nNotebook\nInteractive analysis,\neducation\nEasy to use and visualize data\nGoogle\nColab\nDeep learning, cloud-\nbased projects\nFree GPU/TPU and no local setup\nVS Code\nLarge projects,\ndebugging\nLightweight with advanced features\nPyCharm\nEnterprise-level,\ncomplex applications\nProfessional IDE with deep features\nCursor AI\nAI-assisted coding,\nproductivity\nAI-enhanced code suggestions\nSpyder\nAcademic research,\nscientific computing\nMATLAB-like interface\nRecommendation: If you\u2019re starting out or want a hassle-free experience, \nAnaconda with Jupyter Notebook is the best choice. For advanced AI and big data\nprojects, Google Colab is an excellent free alternative. For robust, large-scale\ndevelopment, VS Code or PyCharm provides advanced capabilities. Since we are\njust starting our learning journey, I will be using Anaconda and Jupyter for the\nmost part of this course\nSummary\nAnaconda + Jupyter Notebook: Best for beginners and interactive analysis.\nGoogle Colab: Ideal for cloud-based work and deep learning.\n1. \n2. \n"}, {"number": "01", "title": "Introduction_to_Data_Science", "page_no": 12, "text": "VS Code: Perfect for integrated, large-scale projects.\nPyCharm: A professional-grade IDE for Python development.\nCursor AI: AI-assisted productivity for fast development.\nChoosing the right tool depends on your project size, complexity, and hardware\nneeds. For most data science workflows, Anaconda with Jupyter Notebook offers\nthe best balance of simplicity, flexibility, and power.\n3. \n4. \n5. \n"}]