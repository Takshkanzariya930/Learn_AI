[{"number": "16", "title": "Introduction to LLMs", "page_no": 1, "text": "Introduction to LLMs (Large Language\nModels)\nLarge Language Models (LLMs) are a breakthrough in artificial intelligence that\nhave revolutionized how machines understand and generate human language.\nThese models are capable of performing a wide range of tasks such as translation,\nsummarization, question answering, and even creative writing \u2014 all by learning\nfrom massive text datasets.\nIn this section, we will build a foundational understanding of what LLMs are, why\nthey matter in data science, and how they differ from traditional machine learning\nmodels.\nWhat is an LLM?\nA Large Language Model is a type of AI model that uses deep learning, specifically\ntransformer architectures, to process and generate natural language. These models\nare \u201clarge\u201d because they contain billions (or even trillions) of parameters \u2014\ntunable weights that help the model make predictions.\nAt their core, LLMs are trained to predict the next word in a sentence, given the\nwords that came before. With enough data and training, they learn complex\nlanguage patterns, world knowledge, and even reasoning skills.\nWhy are LLMs Important?\nVersatility: One LLM can perform dozens of tasks without needing task-\nspecific training.\nZero-shot and few-shot learning: LLMs can handle tasks they\u2019ve never\nexplicitly seen before, based on prompts or examples.\n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 2, "text": "Human-like generation: They produce text that is often indistinguishable from\nhuman writing.\nFoundation for AI applications: They power modern tools like ChatGPT,\nCopilot, Bard, Claude, and more.\nHow are LLMs Different from Traditional ML Models?\nFeature\nTraditional ML\nModels\nLLMs\nInput\nStructured data\nNatural language (text)\nTraining\nTask-specific\nGeneral pretraining on large\ntext\nParameters\nThousands to\nmillions\nBillions to trillions\nAdaptability\nLimited\nHighly adaptable via prompting\nKnowledge\nrepresentation\nFeature-\nengineered\nImplicit via word embeddings\nWhere are LLMs Used?\nLLMs are widely used across industries:\nCustomer support: Chatbots and automated help desks\nEducation: AI tutors, personalized learning\nHealthcare: Clinical documentation and patient interaction\nSoftware Development: Code generation and bug detection\nCreative fields: Story writing, poetry, music lyrics\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 3, "text": "History of LLMs\nUnderstanding the history of Large Language Models (LLMs) helps us appreciate\nhow far we\u2019ve come in natural language processing (NLP) and the innovations that\nmade today\u2019s AI systems possible.\nThis section walks through the key milestones \u2014 from early statistical models to\nthe modern transformer revolution.\nEarly NLP Approaches\nBefore LLMs, language tasks were handled using:\nRule-based systems: Manually written logic for grammar and syntax.\nStatistical models: Such as n-gram models, which predicted the next word\nbased on a fixed window of previous words.\nBag-of-words and TF-IDF: Used for basic text classification but ignored word\norder and context.\nThese models worked for simple tasks, but failed to capture deeper meaning,\nsemantics, or long-range dependencies in language.\nThe Rise of Neural Networks\nWith the rise of deep learning, models began learning richer representations:\nWord Embeddings like Word2Vec (2013) and GloVe (2014) mapped words to\ncontinuous vector spaces.\nRecurrent Neural Networks (RNNs) and LSTMs enabled models to process\nsequences, but they struggled with long texts and parallel processing.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 4, "text": "Transformers: The Game Changer\nIn 2017, Google introduced the Transformer architecture in the paper \u201cAttention is\nAll You Need.\u201d\nKey features of transformers:\nSelf-attention mechanism allows the model to weigh the importance of\ndifferent words, regardless of their position.\nEnables parallelization, making training on massive datasets feasible.\nThis led to a new generation of LLMs:\nModel\nYear\nKey Contribution\nBERT\n2018\nBidirectional context understanding\nGPT-1\n2018\nIntroduced unidirectional generation\nGPT-2\n2019\nGenerated coherent long-form text\nT5\n2020\nUnified text-to-text framework\nGPT-3\n2020\n175B parameters, capable of few-shot\nlearning\nChatGPT / GPT-3.5 /\nGPT-4\n2022\u2013\n2023\nConversational abilities, better\nreasoning\nClaude, Gemini, LLaMA,\nMistral\n2023+\nOpen-source and scalable\nalternatives\nPretraining & Finetuning\nThe modern LLM pipeline consists of:\nPretraining on a large corpus of general text (e.g., books, Wikipedia, web\npages).\nFinetuning for specific tasks (e.g., summarization, coding help).\n\u2022 \n\u2022 \n1. \n2. \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 5, "text": "Reinforcement Learning from Human Feedback (RLHF) \u2014 used to make\nmodels safer and more helpful (e.g., ChatGPT).\nSummary\nThe evolution of LLMs is a story of scale, data, and architecture. The shift from\nhandcrafted rules to deep neural transformers has allowed machines to understand\nand generate language with remarkable fluency.\n3. \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 6, "text": "How LLMs Work\nIn this section, we break down the inner workings of Large Language Models\n(LLMs). While these models seem like magic from the outside, they are grounded in\nfundamental machine learning and deep learning principles \u2014 especially the\ntransformer architecture.\nWe\u2019ll go through how LLMs process text, represent meaning, and generate\ncoherent outputs.\nTokenization: Breaking Text into Units\nLLMs do not process text as raw strings. Instead, they break input text into smaller\nunits called tokens. Tokens can be:\nWhole words (for simple models)\nSubwords (e.g., \u201cun\u201d + \u201cbeliev\u201d + \u201cable\u201d)\nCharacters (rare for LLMs, used in specific domains)\nThis process helps reduce the vocabulary size and handle unknown or rare words\nefficiently.\nPopular tokenizers include Byte-Pair Encoding (BPE) and SentencePiece.\nEmbeddings: Converting Tokens to Vectors\nOnce text is tokenized, each token is mapped to a high-dimensional vector\nthrough an embedding layer. These embeddings capture relationships between\nwords based on context.\nFor example, the words \u201cking\u201d and \u201cqueen\u201d will be closer in the embedding space\nthan unrelated words like \u201cbanana\u201d or \u201ccar\u201d.\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 7, "text": "The Transformer Architecture\nThe core of LLMs is the transformer, introduced in 2017. It replaced earlier models\nlike RNNs and LSTMs by allowing for better performance and scalability.\nKey components of a transformer:\nSelf-Attention Mechanism: Enables the model to focus on different parts of\nthe input when processing each token. For example, in the sentence \u201cThe cat\nsat on the mat,\u201d the word \u201csat\u201d may attend more to \u201ccat\u201d and \u201cmat\u201d than to\n\u201cthe\u201d.\nMulti-Head Attention: Allows the model to capture different types of\nrelationships simultaneously.\nFeedforward Networks: Add depth and complexity to the model.\nPositional Encoding: Since transformers process all tokens in parallel, they\nneed a way to encode the order of tokens.\nThese components are stacked in layers \u2014 more layers typically mean more\nmodeling power.\nTraining LLMs: Predicting the Next Token\nLLMs are trained using a simple but powerful objective: predict the next token\ngiven the previous tokens.\nFor example:\nInput: \u201cThe sun rises in the\u201d\nOutput: \u201ceast\u201d\nThe model adjusts its internal weights using a large dataset and gradient descent\nto minimize prediction error. Over billions of examples, the model learns grammar,\nfacts, reasoning patterns, and even basic common sense.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 8, "text": "This process is known as causal language modeling in models like GPT. Other\nmodels like BERT use masked language modeling, where random tokens are\nhidden and the model must predict them.\nGeneration: Producing Human-like Text\nOnce trained, the model can generate text by predicting one token at a time:\nStart with an input prompt.\nPredict the next token based on context.\nAppend the new token to the prompt.\nRepeat until a stopping condition is met.\nSeveral sampling strategies control the output:\nGreedy decoding: Always choose the most likely next token.\nBeam search: Explore multiple token sequences in parallel.\nTop-k / top-p sampling: Add randomness for more creative or diverse\noutputs.\nLimitations of LLMs\nDespite their capabilities, LLMs have limitations:\nNo true understanding: They learn patterns, not meaning.\nHallucinations: They can generate plausible but false information.\nBias: Trained on large web corpora, they can inherit societal biases.\nCompute-intensive: Training and running LLMs requires significant hardware\nresources.\n1. \n2. \n3. \n4. \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 9, "text": "Introduction to RAG-based Systems\nAs Large Language Models (LLMs) become central to modern AI applications, a key\nlimitation remains: they don\u2019t know anything beyond their training data. They\ncannot access up-to-date information or internal company documents unless\nexplicitly provided.\nThis is where RAG (Retrieval-Augmented Generation) systems come in. RAG\nbridges the gap between language generation and external knowledge, making\nLLMs more accurate, dynamic, and context-aware.\nWhat is a RAG System?\nRetrieval-Augmented Generation (RAG) is an AI architecture that combines:\nA retriever \u2013 to search a knowledge base for relevant documents or facts.\nA generator (LLM) \u2013 to synthesize a response using both the retrieved content\nand the input question.\nRather than generating answers purely from internal memory (which may be\noutdated or incomplete), a RAG system fetches real documents and grounds the\nmodel\u2019s output in that information.\nWhy Use RAG?\nRAG addresses several key challenges of LLMs:\nProblem in LLMs\nHow RAG Helps\nHallucination (made-up\nfacts)\nAnchors generation in real data\n1. \n2. \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 10, "text": "Problem in LLMs\nHow RAG Helps\nOutdated knowledge\nUses fresh, external sources like databases or\nwebsites\nLimited context window\nDynamically injects only the most relevant info\nDomain-specific needs\nConnects model to private corpora, PDFs, etc.\nBasic Workflow of a RAG System\nUser query \u2192\nRetriever fetches relevant documents (e.g., from a vector database) \u2192\nDocuments + query are passed to the LLM \u2192\nLLM generates a grounded, accurate response.\nThis loop allows the model to act more like a researcher with access to a\nsearchable library.\nComponents of a RAG Pipeline\nEmbedding Model: Converts text into dense vectors to enable similarity\nsearch.\nVector Store: A searchable index (e.g., FAISS, Weaviate, Pinecone) where\ndocument embeddings are stored.\nRetriever: Queries the vector store with the input to find top-k most similar\ndocuments.\nLLM: Uses the retrieved content and prompt to generate a final response.\nOptional Reranker: Improves retrieval quality by reordering results.\n1. \n2. \n3. \n4. \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "16", "title": "Introduction to LLMs", "page_no": 11, "text": "Example Use Cases\nEnterprise chatbots: Pull answers from internal documents and manuals.\nCustomer support: Query knowledge bases in real-time.\nAcademic research tools: Generate summaries grounded in actual papers.\nHealthcare assistants: Retrieve clinical guidelines or patient history for\npersonalized advice.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}]