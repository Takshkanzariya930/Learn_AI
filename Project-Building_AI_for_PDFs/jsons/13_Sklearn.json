[{"number": "13", "title": "Sklearn", "page_no": 1, "text": "Machine Learning Problem-Solving\nSteps\nIn order to solve machine learning problems, we follow a structured approach that\nhelps ensure accuracy, clarity, and effectiveness. Here are the main steps involved:\n1. Look at the Big Picture\nUnderstand the overall problem you\u2019re solving. Define your objective clearly \u2014\nwhat does success look like?\n2. Get the Data\nCollect relevant and quality data from reliable sources. Without data, there\u2019s no\nmachine learning.\n3. Explore and Visualize the Data\nAnalyze and visualize data to uncover patterns, trends, and anomalies. This step\nhelps you understand what you\u2019re working with.\n4. Prepare the Data\nClean, transform, and format the data. Handle missing values, normalize features,\nand split the data into training and testing sets.\n5. Select a Model and Train It\nChoose a suitable machine learning algorithm and train it using your data. This is\nwhere your model learns from patterns.\n"}, {"number": "13", "title": "Sklearn", "page_no": 2, "text": "6. Fine-Tune Your Model\nOptimize hyperparameters, try different techniques, and improve performance\nthrough iteration.\n7. Present Your Solution\nExplain your model\u2019s results using visuals, metrics, and clear language so\nstakeholders can understand and make decisions.\n8. Launch, Monitor, and Maintain\nDeploy the model in the real world, monitor its performance, and update it\nregularly as new data arrives.\n"}, {"number": "13", "title": "Sklearn", "page_no": 3, "text": "Datasets for Machine Learning\nMachine Learning requires quality datasets for training and testing models. Some\npopular sources include:\nOpenML.org \u2013 A collaborative platform offering a wide range of datasets with\nmetadata and tools for benchmarking.\nUCI Machine Learning Repository \u2013 One of the oldest and most widely used\nsources for machine learning datasets.\nKaggle \u2013 A data science community offering large-scale, real-world datasets\nand competitions.\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 4, "text": "Quick Training with Scikit-learn (CSV\nData)\nThis guide walks you through training a model on your own CSV dataset using\nscikit-learn.\n1. Import Libraries\n2. Load Your CSV File\n3. Separate Features and Label\n4. Train the Model\nThe model is now trained on your complete dataset.\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndata = pd.read_csv('data.csv')\n# Replace with your actual CSV file path\nX = data.iloc[:, :-1]\n# All columns except the last as features\ny = data.iloc[:, -1]\n# Last column as label\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n"}, {"number": "13", "title": "Sklearn", "page_no": 5, "text": "5. Inference\npredictions = model.predict(X)\n# Predict on the same/new data (for demonstration)\nprint(predictions)\n# Display predictions\n"}, {"number": "13", "title": "Sklearn", "page_no": 6, "text": "Evaluating Performance of ML Models\nWhen we build a machine learning model, especially a classification model (which\npredicts categories like \u201cspam\u201d or \u201cnot spam\u201d, \u201cdog\u201d or \u201ccat\u201d), it\u2019s important to\nmeasure how well the model is performing.\nOne of the most basic ways to evaluate a classification model is accuracy.\nWhat is Accuracy?\nIn simple terms, accuracy tells us how often the model was right.\nIf you gave your model 100 questions to answer, and it got 90 of them correct, its\naccuracy would be:\nSo, accuracy is just the fraction of predictions the model got right.\nEvaluating Accuracy\nAccuracy = (Correct Predictions) / (Total Predictions)\nThis is the simplest and most intuitive way to check if the model is doing a good\njob.\nAccuracy = Correct Predictions / Total Predictions = 90 / 100 = 90%\n"}, {"number": "13", "title": "Sklearn", "page_no": 7, "text": "Gurgaon House Price Prediction Model\nWhy Are We Building This?\nGurgaon, a rapidly growing city in India, has seen a sharp rise in real estate\ndevelopment over the past decade. With its proximity to Delhi, booming IT hubs,\nand modern infrastructure, Gurgaon has become a major attraction for both\nhomebuyers and investors. However, the real estate market here is highly dynamic\nand often difficult to assess without proper data-driven tools.\nWe\u2019re building a Gurgaon house price prediction model to help:\nUnderstand how factors like location, size, number of rooms, and amenities\naffect property prices in Gurgaon.\nAssist buyers in identifying fair prices based on historical trends.\nHelp sellers estimate an appropriate asking price.\nEmpower real estate agents and platforms to improve recommendations and\nnegotiations.\nHow Will We Build It?\nWhile we don\u2019t have access to a large, clean dataset of house prices in Gurgaon\nright now, we will use a well-known and cleaned dataset\u2014the California housing\ndataset\u2014as a proxy. This will allow us to build, test, and evaluate a working model\nwith real-world variables like:\nMedian income of the area\nProximity to the city center\nNumber of rooms\nLatitude and longitude\nPopulation density\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 8, "text": "We\u2019ll treat this as a simulation: suppose the California data is Gurgaon data, and\nsuppose we are building this model for a neighborhood where both you and I live\nor work nearby.\nOnce the model is developed and understood, we can later adapt the same\napproach to real Gurgaon data when available, using the same techniques and\nlogic.\n"}, {"number": "13", "title": "Sklearn", "page_no": 9, "text": "Revisiting Steps to solve this problem\nUnderstanding the Problem\nBefore we build the model, we need to understand what kind of machine learning\nproblem we are solving.\nFirst, we\u2019ll check if this is a supervised or unsupervised learning task. Since we\nhave historical data with house prices (our target), and we want to predict the price\nof a house based on input features, this is a supervised learning problem.\nNext, we observe that the model is predicting one continuous label \u2014 the price of\na house \u2014 based on several input features. This makes it a univariate regression\nproblem.\n"}, {"number": "13", "title": "Sklearn", "page_no": 10, "text": "Measuring Errors (RMSE & MAE)\nAfter training our regression model, we need to evaluate how good its predictions\nare. Two common metrics used for this are MAE and RMSE.\n1. Mean Absolute Error (MAE)\nMAE stands for Mean Absolute Error. It calculates the average of the absolute\ndifferences between the predicted and actual values.\nFormula: MAE = (1/n) \u00d7 sum of |actual - predicted|\nIt treats all errors equally, no matter their size.\nMAE is based on the Manhattan norm (also called L1 norm), which measures\ndistance by summing absolute values.\n2. Root Mean Squared Error (RMSE)\nRMSE stands for Root Mean Squared Error. It calculates the square root of the\naverage of squared differences between predicted and actual values.\nFormula: RMSE = sqrt((1/n) \u00d7 sum of (actual - predicted)\u00b2)\nRMSE gives more weight to larger errors because it squares them.\nRMSE is based on the Euclidean norm (also called L2 norm), which measures\nstraight-line distance.\nSummary\nUse MAE when all errors should be treated equally.\nUse RMSE when larger errors should be penalized more.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 11, "text": "Analyzing the Data (EDA)\nExploratory Data Analysis (EDA) is the process of examining a dataset to\nsummarize its main characteristics, often using visual methods or quick commands.\nThe goal is to understand the structure of the data, detect patterns, spot\nanomalies, and get a feel for what kind of preprocessing or modeling might be\nneeded.\nFor our project, we\u2019ll perform EDA on the California housing dataset (which we are\ntreating as if it represents Gurgaon data). Here are some key commands we\u2019ll use:\n1. df.head()\nDisplays the first 5 rows of the dataset.\nUseful for getting a quick overview of what the data looks like \u2014 column\nnames, data types, and sample values.\n2. df.info()\nGives a summary of the dataset.\nShows the number of entries, column names, data types, and how many non-\nnull values each column has.\nHelps us identify missing values or incorrect data types.\n3. df.describe()\nProvides statistical summaries for numeric columns.\nShows:\nCount: Total number of non-null entries\nMean: Average value\nStd: Standard deviation\nMin: The smallest value (0th percentile or 1st quartile in some contexts)\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 12, "text": "25%: The 1st quartile (Q1) \u2014 25% of the data is below this value\n50%: The median or 2nd quartile (Q2) \u2014 half of the data is below this\nvalue\n75%: The 3rd quartile (Q3) \u2014 75% of the data is below this value\nMax: The largest value (often considered the 4th quartile or 100th\npercentile)\nPercentiles divide the data into 100 equal parts. Quartiles divide the data into\n4 equal parts (Q1 = 25th percentile, Q2 = 50th, Q3 = 75th). So:\nMin is the 0th percentile\nMax is the 100th percentile\nThis helps us understand how the values are spread out and if there are outliers.\n4. df['column_name'].value_counts()\nShows the count of each unique value in a specific column.\nUseful for categorical columns to see how values are distributed.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 13, "text": "Creating a Test Set\nWhen building machine learning models, one of the most important steps is \nsplitting your dataset into training and test sets. This ensures your model is\nevaluated on data it has never seen before, which is critical for assessing its ability\nto generalize.\nThe Problem of Data Snooping Bias\nData snooping bias occurs when information from the test set leaks into the\ntraining process. This can lead to overly optimistic performance metrics and\nmodels that don\u2019t perform well in real-world scenarios.\nTo avoid this, the test set must be isolated before any data exploration, feature\nselection, or model training begins.\nRandom Sampling: A Basic Approach\nA simple method to split the data is to randomly shuffle it and then divide it:\nSetting the random seed (e.g., with np.random.seed(42) ) ensures consistency\nacross runs \u2014 this is crucial for debugging and comparing models fairly.\nimport numpy as np\ndef shuffle_and_split_data(data, test_ratio):\n    np.random.seed(42)\n# Set the seed for reproducibility\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\nreturn data.iloc[train_indices], data.iloc[test_indices]\n"}, {"number": "13", "title": "Sklearn", "page_no": 14, "text": "However, pure random sampling might not always be reliable, especially if the\ndataset contains important patterns that are not evenly distributed.\nStratified Sampling\nTo ensure that important characteristics of the population are well represented in\nboth the training and test sets, we use stratified sampling.\nWhat is a Strata?\nA strata is a subgroup of the data defined by a specific attribute. Stratified\nsampling ensures that each of these subgroups is proportionally represented.\nFor example, in the California housing dataset, median income is a strong\npredictor of house prices. Instead of randomly sampling, we can create strata\nbased on income levels (e.g., binning median income into categories) and ensure\nthe test set maintains the same distribution of income levels as the full dataset.\nCreating Income Categories\nThis code creates a new column income_cat  that categorizes the median_income\ninto five bins. Each bin represents a range of income levels, allowing us to stratify\nour sampling based on these categories.\nWe can plot these income categories to visualize the distribution:\nimport pandas as pd\n# Load the dataset\ndata = pd.read_csv(\"housing.csv\")\n# Create income categories\ndata[\"income_cat\"] = pd.cut(data[\"median_income\"],\n                             bins=[0, 1.5, 3.0, 4.5, 6.0, np.inf],\n                             labels=[1, 2, 3, 4, 5])\nimport matplotlib.pyplot as plt\ndata[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title(\"Income Categories Distribution\")\nplt.xlabel(\"Income Category\")\n"}, {"number": "13", "title": "Sklearn", "page_no": 15, "text": "Stratified Shuffle Split in Scikit-Learn\nScikit-learn provides a built-in way to perform stratified sampling using \nStratifiedShuffleSplit .\nHere\u2019s how you can use it:\nThis ensures that the income distribution in both sets is similar to that of the full\ndataset, reducing sampling bias and making your model evaluation more reliable.\nplt.ylabel(\"Number of Instances\")\nplt.show()\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Assume income_cat is a column in the dataset created from median_income\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"income_cat\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]\n"}, {"number": "13", "title": "Sklearn", "page_no": 16, "text": "Data Visualization\nBefore handling missing values or training models, it\u2019s important to visualize the\ndata to uncover patterns, relationships, and potential issues.\nGeographical Scatter Plot\nVisualize the geographical distribution of the data:\nalpha=0.2  makes overlapping points more visible.\nThis helps reveal data clusters and high-density areas like coastal regions.\nCorrelation Matrix\nTo understand relationships between numerical features, compute the correlation\nmatrix:\nCheck how strongly each attribute correlates with the target:\nThis helps identify useful predictors. For example, median_income  usually shows a\nstrong positive correlation with house prices.\ndf.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\nplt.show()\n\u2022 \n\u2022 \ncorr_matrix = df.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n"}, {"number": "13", "title": "Sklearn", "page_no": 17, "text": "Scatter Matrix\nPlot selected features to see pairwise relationships:\nThis gives an overview of which features are linearly related and may be good\npredictors.\nFocused Income vs Price Plot\nPlot median_income  vs median_house_value  directly:\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median\n_age\"]\nscatter_matrix(df[attributes], figsize=(12, 8))\nplt.show()\ndf.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1,\ngrid=True)\n"}, {"number": "13", "title": "Sklearn", "page_no": 18, "text": "Further Preprocessing & Handling\nMissing Data\nBefore feeding your data into a machine learning algorithm, you need to clean and\nprepare it.\nPrepare Data for Training\nIt\u2019s best to write transformation functions instead of applying them manually. This\nensures:\nReproducibility on any dataset\nReusability across projects\nCompatibility with live systems\nEasier experimentation\nStart by creating a clean copy and separating the predictors and labels:\nHandling Missing Data\nSome features, like total_bedrooms , contain missing values. You can:\nDrop rows with missing values\nDrop the entire column\nImpute missing values (recommended)\nWe\u2019ll use option 3 using SimpleImputer  from Scikit-Learn, which allows consistent\nhandling across all datasets (train, test, new data):\n\u2022 \n\u2022 \n\u2022 \n\u2022 \nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n1. \n2. \n3. \n"}, {"number": "13", "title": "Sklearn", "page_no": 19, "text": "This computes the median for each numerical column and stores it in \nimputer.statistics_ :\nNow apply the learned medians to transform the data:\nOther available strategies:\n\"mean\"  \u2013 replaces with mean value\n\"most_frequent\"  \u2013 for the most common value (can handle categorical)\n\"constant\"  \u2013 fill with a fixed value using fill_value=...\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nhousing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\n>>> imputer.statistics_\narray([-118.51 ,\n34.26 ,\n29. , 2125. , 434. , 1167. , 408. , 3.5385])\nX = imputer.transform(housing_num)\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 20, "text": "Scikit-Learn Design Principles\nScikit-Learn has a simple and consistent API that makes it easy to use and\nunderstand. Below are the key design principles behind it:\n1. Consistency\nAll objects follow a standard interface, which makes learning and using different\ntools in Scikit-Learn easier.\n2. Estimators\nAny object that learns from data is called an estimator.\nUse the .fit()  method to train an estimator.\nIn supervised learning, pass both X  (features) and y  (labels) to .fit(X, y) .\nHyperparameters (like strategy='mean'  in SimpleImputer ) are set when\ncreating the object.\nExample:\n\u2022 \n\u2022 \n\u2022 \nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(data)\n"}, {"number": "13", "title": "Sklearn", "page_no": 21, "text": "3. Transformers\nSome estimators can also transform data. These are called transformers.\nUse .transform()  to apply the transformation after fitting.\nUse .fit_transform()  to do both in one step.\nExample:\n4. Predictors\nModels that can make predictions are predictors.\nUse .predict()  to make predictions on new data.\nUse .score()  to evaluate performance (e.g., accuracy or R\u00b2).\nExample:\n5. Inspection\nHyperparameters can be accessed directly: model.param_name\nLearned parameters are stored with an underscore: model.coef_ , \nimputer.statistics_\n\u2022 \n\u2022 \nX_transformed = imputer.fit_transform(data)\n\u2022 \n\u2022 \nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nscore = model.score(X_test, y_test)\n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 22, "text": "6. No Extra Classes\nInputs and outputs are basic structures like NumPy arrays or Pandas\nDataFrames.\nNo need to learn custom data types.\n7. Composition\nYou can combine steps into a Pipeline, chaining transformers and a final predictor.\nExample:\n8. Sensible Defaults\nMost tools in Scikit-Learn work well with default settings, so you can get started\nquickly.\nNote on DataFrames\nEven if you input a Pandas DataFrame, the output of transformers like \ntransform()  will be a NumPy array. You can convert it back like this:\n\u2022 \n\u2022 \npipeline = Pipeline([\n(\"imputer\", SimpleImputer(strategy=\"median\")),\n(\"model\", LinearRegression())\n])\npipeline.fit(X, y)\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n"}, {"number": "13", "title": "Sklearn", "page_no": 23, "text": "Handling Categorical and Text\nAttributes in Scikit-Learn\nMost machine learning algorithms work best with numerical data. But real-world\ndatasets often contain categorical or text attributes. Let\u2019s understand how to\nhandle these in Scikit-Learn using the ocean_proximity  column from the\nCalifornia housing dataset as an example.\n1. Categorical Attributes\nText columns like \"ocean_proximity\"  are not free-form text but limited to a fixed\nset of values (e.g., \"NEAR BAY\" , \"INLAND\" ). These are known as categorical\nattributes.\nExample:\n2. Ordinal Encoding\nScikit-Learn\u2019s OrdinalEncoder  can convert categories to numbers:\nThis will output a 2D NumPy array with numerical category codes.\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head()\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n"}, {"number": "13", "title": "Sklearn", "page_no": 24, "text": "To see the mapping:\n\u26a0\ufe0f Caution: Ordinal encoding implies an order between categories, which may not\nbe true here. For example, it treats INLAND (1)  as closer to <1H OCEAN (0)  than \nNEAR OCEAN (4) , which might not make sense.\n3. One-Hot Encoding\nFor unordered categories, one-hot encoding is a better choice. It creates one\nbinary column per category.\nThis gives a sparse matrix (efficient storage for mostly zeros).\nTo convert it to a regular NumPy array:\nOr directly get a dense array:\nTo check category order:\nordinal_encoder.categories_\n# Output: array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'])\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot.toarray()\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\ncat_encoder.categories_\n"}, {"number": "13", "title": "Sklearn", "page_no": 25, "text": "4. Summary\nMethod\nUse When\nOutput Type\nOrdinalEncoder\nCategories have an order\n2D NumPy array\nOneHotEncoder\nCategories are unordered\nSparse or dense\nUsing the right encoding ensures your model learns correctly from categorical\nfeatures.\n"}, {"number": "13", "title": "Sklearn", "page_no": 26, "text": "Feature Scaling and Transformation\nFeature scaling is a crucial preprocessing step. Most machine learning algorithms\nperform poorly when input features have vastly different scales.\nIn the California housing dataset, for example:\ntotal_rooms  ranges from 6 to over 39,000\nmedian_income  ranges from 0 to 15\nIf you don\u2019t scale these features, models will give more importance to \ntotal_rooms  simply because it has larger values.\nWhy Scaling Is Needed\nMany models (like Linear Regression, KNN, SVMs, Gradient Descent-based\nalgorithms) assume features are on a similar scale.\nWithout scaling, features with larger ranges can dominate model behavior.\nScaling makes training more stable and faster.\nMin-Max Scaling (Normalization)\nThis method rescales the data to a specific range, usually [0, 1]  or [-1, 1] .\nFormula:\nUse Scikit-Learn\u2019s MinMaxScaler :\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \nscaled_value = (x - min) / (max - min)\nfrom sklearn.preprocessing import MinMaxScaler\n"}, {"number": "13", "title": "Sklearn", "page_no": 27, "text": "Use feature_range=(-1, 1)  for models like neural networks.\nSensitive to outliers \u2014 extreme values can distort the scale.\nStandardization (Z-score Scaling)\nThis method centers the data around 0 and scales it based on standard deviation.\nFormula:\nUse Scikit-Learn\u2019s StandardScaler :\nResulting features have zero mean and unit variance\nRobust to outliers compared to min-max scaling\nRecommended for most ML algorithms, especially when using gradient\ndescent\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n\u2022 \n\u2022 \nstandardized_value = (x - mean) / std\nfrom sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 28, "text": "Transformation Pipelines\nAs datasets grow more complex, data preprocessing often involves multiple steps\nsuch as imputing missing values, scaling features, encoding categorical variables,\netc. These steps must be applied in the correct order and consistently across\ntraining, validation, test, and future production data.\nTo streamline this process, Scikit-Learn provides the Pipeline  class \u2014 a powerful\nutility for chaining data transformations.\nBuilding a Numerical Pipeline\nA typical pipeline for numerical attributes might include:\nImputation of missing values (e.g., with median).\nFeature scaling (e.g., with standardization).\nHow It Works\nThe pipeline takes a list of steps as (name, transformer)  pairs.\nNames must be unique and should not contain double underscores __ .\nAll intermediate steps must be transformers (i.e., must implement \nfit_transform() ).\n1. \n2. \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n(\"impute\", SimpleImputer(strategy=\"median\")),\n(\"standardize\", StandardScaler()),\n])\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 29, "text": "The final step can be either a transformer or a predictor.\nUsing make_pipeline\nIf you don\u2019t want to name the steps manually, you can use make_pipeline() :\nThis automatically names the steps using the class names in lowercase.\nIf the same class appears multiple times, a number is appended (e.g., \nstandardscaler-1 ).\nApplying the Pipeline\nCall fit_transform()  to apply all transformations in sequence:\nExample output:\nEach row corresponds to a transformed sample.\nEach column corresponds to a scaled feature.\nRetrieving Feature Names\nTo turn the result back into a DataFrame with feature names:\n\u2022 \nfrom sklearn.pipeline import make_pipeline\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\u2022 \n\u2022 \nhousing_num_prepared = num_pipeline.fit_transform(housing_num)\nprint(housing_num_prepared[:2].round(2))\narray([[-1.42,\n1.01,\n1.86,\n0.31,\n1.37,\n0.14,\n1.39, -0.94],\n[ 0.60, -0.70,\n0.91, -0.31, -0.44, -0.69, -0.37,\n1.17]])\n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 30, "text": "Pipeline as a Transformer or Predictor\nIf the last step is a transformer, the pipeline behaves like a transformer\n( fit_transform() , transform() ).\nIf the last step is a predictor (e.g., a model), the pipeline behaves like an\nestimator ( fit() , predict() ).\nThis flexibility makes Pipeline  the standard way to handle data preprocessing\nand modeling in Scikit-Learn projects.\ndf_housing_num_prepared = pd.DataFrame(\n    housing_num_prepared,\n    columns=num_pipeline.get_feature_names_out(),\n    index=housing_num.index\n)\n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 31, "text": "Data Preprocessing - Final Pipeline\nIn this section, we will consolidate everything we\u2019ve done so far into one final\nscript using Scikit-Learn pipelines. This includes:\nCreating a stratified test set\nHandling missing values\nEncoding categorical variables\nScaling numerical features\nCombining everything using Pipeline  and ColumnTransformer\nThis will ensure clean, modular, and reproducible code \u2014 perfect for production\nand education.\nFinal Preprocessing Code using Scikit-Learn Pipelines\n1. \n2. \n3. \n4. \n5. \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.preprocessing import OrdinalEncoder  # Uncomment if you prefer \nordinal\n# 1. Load the data\nhousing = pd.read_csv(\"housing.csv\")\n# 2. Create a stratified test set based on income category\nhousing[\"income_cat\"] = pd.cut(\n    housing[\"median_income\"],\n    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n"}, {"number": "13", "title": "Sklearn", "page_no": 32, "text": "    labels=[1, 2, 3, 4, 5]\n)\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index].drop(\"income_cat\", axis=1)\n    strat_test_set = housing.loc[test_index].drop(\"income_cat\", axis=1)\n# Work on a copy of training data\nhousing = strat_train_set.copy()\n# 3. Separate predictors and labels\nhousing_labels = housing[\"median_house_value\"].copy()\nhousing = housing.drop(\"median_house_value\", axis=1)\n# 4. Separate numerical and categorical columns\nnum_attribs = housing.drop(\"ocean_proximity\", axis=1).columns.tolist()\ncat_attribs = [\"ocean_proximity\"]\n# 5. Pipelines\n# Numerical pipeline\nnum_pipeline = Pipeline([\n(\"imputer\", SimpleImputer(strategy=\"median\")),\n(\"scaler\", StandardScaler()),\n])\n# Categorical pipeline\ncat_pipeline = Pipeline([\n# (\"ordinal\", OrdinalEncoder())  # Use this if you prefer ordinal encoding\n(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n# Full pipeline\nfull_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", cat_pipeline, cat_attribs),\n])\n# 6. Transform the data\nhousing_prepared = full_pipeline.fit_transform(housing)\n"}, {"number": "13", "title": "Sklearn", "page_no": 33, "text": "# housing_prepared is now a NumPy array ready for training\nprint(housing_prepared.shape)\n"}, {"number": "13", "title": "Sklearn", "page_no": 34, "text": "Training and Evaluating ML Models\nNow that our data is preprocessed, let\u2019s move on to training machine learning\nmodels and evaluating their performance. We\u2019ll start with:\nLinear Regression\nDecision Tree Regressor\nRandom Forest Regressor\nWe\u2019ll first test them on the training data and then use cross-validation to get a\nbetter estimate of their true performance.\n1. Train and Test Models on the Training Set\n\u2022 \n\u2022 \n\u2022 \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n# Linear Regression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n# Decision Tree\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)\n# Random Forest\nforest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)\n# Predict using training data\nlin_preds = lin_reg.predict(housing_prepared)\n"}, {"number": "13", "title": "Sklearn", "page_no": 35, "text": "A Warning About Training RMSE\nTraining RMSE only shows how well the model fits the training data. It does not\ntell us how well it will perform on unseen data. In fact, the Decision Tree and\nRandom Forest may overfit, leading to very low training error but poor\ngeneralization.\n2. Cross-Validation: A Better Evaluation Strategy\nCross-validation helps us evaluate how a model generalizes to new data without\nneeding to touch the test set.\nWhat is Cross-Validation?\nInstead of training the model once and evaluating on a holdout set, k-fold cross-\nvalidation splits the training data into k folds (typically 10), trains the model on k-1\nfolds, and validates it on the remaining fold. This process repeats k times.\nWe\u2019ll use cross_val_score  from sklearn.model_selection .\ntree_preds = tree_reg.predict(housing_prepared)\nforest_preds = forest_reg.predict(housing_prepared)\n# Calculate RMSE\nlin_rmse = mean_squared_error(housing_labels, lin_preds, squared=False)\ntree_rmse = mean_squared_error(housing_labels, tree_preds, squared=False)\nforest_rmse = mean_squared_error(housing_labels, forest_preds, squared=False)\nprint(\"Linear Regression RMSE:\", lin_rmse)\nprint(\"Decision Tree RMSE:\", tree_rmse)\nprint(\"Random Forest RMSE:\", forest_rmse)\n"}, {"number": "13", "title": "Sklearn", "page_no": 36, "text": "Cross-Validation on Decision Tree\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\n# Evaluate Decision Tree with cross-validation\ntree_rmses = -cross_val_score(\n    tree_reg,\n    housing_prepared,\n    housing_labels,\n    scoring=\"neg_root_mean_squared_error\",\n    cv=10\n)\n# WARNING: Scikit-Learn\u2019s scoring uses utility functions (higher is better), so \nRMSE is returned as negative.\n# We use minus (-) to convert it back to positive RMSE.\nprint(\"Decision Tree CV RMSEs:\", tree_rmses)\nprint(\"\\nCross-Validation Performance (Decision Tree):\")\nprint(pd.Series(tree_rmses).describe())\n"}, {"number": "13", "title": "Sklearn", "page_no": 37, "text": "Model Persistence and Inference with\nJoblib in a Random Forest Pipeline\nLets now summarize how to train a Random Forest model on California housing\ndata, save the model and preprocessing pipeline using joblib , and reuse the\nmodel later for inference on new data ( input.csv ). This approach helps avoid\nretraining the model every time, improving performance and enabling\nreproducibility.\nWhy These Steps?\n1. Why Train Once and Save?\nTraining models repeatedly is time-consuming and computationally\nexpensive.\nSaving the model ( model.pkl ) and preprocessing pipeline ( pipeline.pkl )\nensures you can quickly load and run inference anytime in the future.\n2. Why Use a Preprocessing Pipeline?\nRaw data needs to be cleaned, scaled, and encoded before model training.\nA Pipeline  automates this transformation and ensures identical\npreprocessing during inference.\n3. Why Use Joblib?\njoblib  efficiently serializes large NumPy arrays (like in sklearn models).\nFaster and more suitable than pickle  for scikit-learn  objects.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 38, "text": "4. Why the If-Else Logic?\nThe program checks if a saved model exists.\nIf not, it trains and saves the model.\nIf it does, it skips training and only runs inference, saving time.\nFull Code\n\u2022 \n\u2022 \n\u2022 \nimport os\nimport pandas as pd\nimport numpy as np\nimport joblib\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nMODEL_FILE = \"model.pkl\"\nPIPELINE_FILE = \"pipeline.pkl\"\ndef build_pipeline(num_attribs, cat_attribs):\n    num_pipeline = Pipeline([\n(\"imputer\", SimpleImputer(strategy=\"median\")),\n(\"scaler\", StandardScaler())\n])\n    cat_pipeline = Pipeline([\n(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n    full_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", cat_pipeline, cat_attribs)\n])\nreturn full_pipeline\n"}, {"number": "13", "title": "Sklearn", "page_no": 39, "text": "if not os.path.exists(MODEL_FILE):\n# TRAINING PHASE\n    housing = pd.read_csv(\"housing.csv\")\n    housing['income_cat'] = pd.cut(housing[\"median_income\"],\n                                   bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n                                   labels=[1, 2, 3, 4, 5])\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, _ in split.split(housing, housing['income_cat']):\n        housing = housing.loc[train_index].drop(\"income_cat\", axis=1)\n    housing_labels = housing[\"median_house_value\"].copy()\n    housing_features = housing.drop(\"median_house_value\", axis=1)\n    num_attribs = housing_features.drop(\"ocean_proximity\", axis=1).columns.tolist()\n    cat_attribs = [\"ocean_proximity\"]\n    pipeline = build_pipeline(num_attribs, cat_attribs)\n    housing_prepared = pipeline.fit_transform(housing_features)\n    model = RandomForestRegressor(random_state=42)\n    model.fit(housing_prepared, housing_labels)\n# Save model and pipeline\n    joblib.dump(model, MODEL_FILE)\n    joblib.dump(pipeline, PIPELINE_FILE)\nprint(\"Model trained and saved.\")\nelse:\n# INFERENCE PHASE\n    model = joblib.load(MODEL_FILE)\n    pipeline = joblib.load(PIPELINE_FILE)\n    input_data = pd.read_csv(\"input.csv\")\n    transformed_input = pipeline.transform(input_data)\n    predictions = model.predict(transformed_input)\n    input_data[\"median_house_value\"] = predictions\n"}, {"number": "13", "title": "Sklearn", "page_no": 40, "text": "Summary\nWith this setup, our ML pipeline is:\nEfficient \u2013 No retraining needed if the model exists.\nReproducible \u2013 Same preprocessing logic every time.\nProduction-ready \u2013 Can be deployed or reused across multiple systems.\n    input_data.to_csv(\"output.csv\", index=False)\nprint(\"Inference complete. Results saved to output.csv\")\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "13", "title": "Sklearn", "page_no": 41, "text": "Conclusion: California Housing Price\nPrediction Project\nIn this project, we built a complete machine learning pipeline to predict California\nhousing prices using various regression algorithms. We started by:\nLoading and preprocessing the dataset ( housing.csv ) with careful treatment\nof missing values, scaling, and encoding using a custom pipeline.\nStratified splitting was used to maintain income category distribution\nbetween train and test sets.\nWe trained and evaluated multiple algorithms including:\nLinear Regression\nDecision Tree Regressor\nRandom Forest Regressor\nThrough cross-validation, we found that Random Forest performed the best,\noffering the lowest RMSE and most stable results.\nFinally, we built a script that:\nTrains the Random Forest model and saves it using joblib .\nUses an if-else logic to skip retraining if the model exists.\nApplies the trained model to new data ( input.csv ) to predict \nmedian_house_value , storing results in output.csv .\nThis pipeline ensures that predictions are accurate, efficient, and ready for\nproduction deployment.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}]