[{"number": "14", "title": "Introduction to Neural Networks", "page_no": 1, "text": "Introduction to Neural Networks\nInspiration from Nature\nBirds inspired humans to build airplanes. The tiny hooks on burrs sticking to a\ndog\u2019s fur led to the invention of Velcro. And just like that, nature has always been\nhumanity\u2019s greatest engineer.\nSo, when it came to making machines that could think, learn, and solve problems,\nwhere did we look?\nTo the human brain.\nThat\u2019s how neural networks were born \u2014 machines inspired by neurons in our\nbrains, built to recognize patterns, make decisions, and even learn from experience.\nWhat is AI, ML, and DL?\nBefore we dive into neural networks, let\u2019s untangle these buzzwords.\nTerm\nStands for\nThink of it as\u2026\nAI\nArtificial Intelligence\nThe big umbrella: making machines \u201csmart\u201d\nML\nMachine Learning\nA subset of AI: machines that learn from\ndata\nDL\nDeep Learning\nA type of ML: uses neural networks\nLet\u2019s simplify:\nAI is the dream: \u201cCan we make machines intelligent?\u201d\nML is the method: \u201cLet\u2019s give machines data and let them learn.\u201d\nDL is the tool: \u201cLet\u2019s use neural networks that learn in layers \u2014 like the brain.\u201d\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 2, "text": "So, when we talk about neural networks, we\u2019re entering the world of deep\nlearning, which is a part of machine learning, which itself is a part of AI.\nSo, What Are Neural Networks?\nImagine a bunch of simple decision-makers called neurons, connected together in\nlayers.\nEach neuron:\nTakes some input (like a number)\nApplies a little math (weights + bias)\nPasses the result through a rule (called an activation function)\nSends the output to the next layer\nBy connecting many of these neurons, we get a neural network.\nAnd what\u2019s amazing?\nEven though each neuron is simple, when combined, the network becomes\npowerful \u2014 like how a bunch of ants can build a complex colony.\nWhy Are Neural Networks Useful?\nBecause they can learn patterns, even when we don\u2019t fully understand the patterns\nourselves.\nExamples:\nRecognize cats in photos\nConvert speech to text\nTranslate languages\nPredict stock prices\nGenerate art\nPower AI like ChatGPT\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 3, "text": "Structure of a Neural Network\nHere\u2019s the basic anatomy of a neural network:\nEach layer is just a bunch of neurons working together. The more hidden layers,\nthe \u201cdeeper\u201d the network. Hence: Deep Learning.\nWait \u2014 Why Not Use Simple Code Instead?\nGood question.\nSometimes, a simple formula or rule is enough (like area = length \u00d7 width ).\nBut what about:\nRecognizing handwritten digits?\nUnderstanding language?\nDiagnosing diseases from X-rays?\nThere are no easy formulas for these. Neural networks learn the formula by\nthemselves from lots of examples.\nHow Do Neural Networks Learn?\nLet\u2019s say the network tries to predict y = x\u00b2 + x .\nIt starts with random guesses (bad predictions)\nIt checks how wrong it is (loss)\nIt adjusts the internal settings (weights) to be a little better\nRepeat, repeat, repeat\u2026\nInput Layer       \u2192       Hidden Layers        \u2192       Output Layer\n(Data)\n(Neurons doing math)\n(Prediction)\n\u2022 \n\u2022 \n\u2022 \n1. \n2. \n3. \n4. \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 4, "text": "Over time, the network figures out the relationship between x and y.\nThis process is called training \u2014 and it\u2019s where the magic happens.\nAre They Really Like the Brain?\nKind of \u2014 but very simplified.\nA biological brain neuron connects to 1000s of others\nIt processes chemicals, spikes, timings\nIt adapts and rewires itself\nA neural network is a mathematical model \u2014 inspired by the brain, but way\nsimpler. Still, the results are powerful.\nSummary\nConcept\nMeaning\nAI\nMaking machines act smart\nML\nLetting machines learn from data\nDL\nUsing multi-layered neural networks to learn complex stuff\nNeural\nNetwork\nA network of artificial neurons that learns from data\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 5, "text": "Perceptron \u2013 The Simplest Neural\nNetwork\nWhat is a Perceptron?\nThe perceptron is the basic building block of a neural network. It\u2019s a simple\ncomputational model that takes several inputs, applies weights to them, adds a\nbias, and produces an output. It\u2019s essentially a decision-making unit.\nReal-life Analogy\nImagine you\u2019re trying to decide whether to go outside based on:\nIs it sunny?\nIs it the weekend?\nAre you free today?\nYou assign importance (weights) to each factor:\nSunny: 0.6\nWeekend: 0.3\nFree: 0.8\nYou combine these factors to make a decision: Go or Not Go.\nThis is what a perceptron does.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 6, "text": "Perceptron Formula\nA perceptron takes inputs (x1, x2, \u2026, xn), multiplies each by its corresponding\nweight (w1, w2, \u2026, wn), adds a bias (b), and passes the result through an activation\nfunction.\ny = f(w1x1 + w2x2 + \u2026 + wn*xn + b)\nWhere:\nxi: input features\nwi: weights\nb: bias\nf: activation function (e.g., step function)\nStep-by-step Example: Binary Classification\nLet\u2019s say we want a perceptron to learn this simple table:\nInput (x1, x2)\nOutput (y)\n(0, 0)\n0\n(0, 1)\n0\n(1, 0)\n0\n(1, 1)\n1\nThis is the behavior of a logical AND gate.\nWe will use:\nInputs: x1, x2\nWeights: w1, w2\nBias: b\nActivation Function: Step function\nStep function:\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 7, "text": "Code: Simple Perceptron from Scratch\nThis matches the AND logic perfectly.\nSummary\nA perceptron is the simplest form of a neural network.\nIt performs a weighted sum of inputs, adds a bias, and passes the result\nthrough an activation function to make a decision.\nIt can model simple binary functions like AND, OR, etc.\ndef step(x):\nreturn 1 if x >= 0 else 0\ndef step(x):\nreturn 1 if x >= 0 else 0\ndef perceptron(x1, x2, w1, w2, b):\n    z = x1 * w1 + x2 * w2 + b\nreturn step(z)\n# Try different weights and bias to match the AND logic\nprint(perceptron(0, 0, 1, 1, -1.5))\n# Expected: 0\nprint(perceptron(0, 1, 1, 1, -1.5))\n# Expected: 0\nprint(perceptron(1, 0, 1, 1, -1.5))\n# Expected: 0\nprint(perceptron(1, 1, 1, 1, -1.5))\n# Expected: 1\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 8, "text": "Common Terms in Deep Learning\nBefore diving into neural networks, let\u2019s clarify some common terms you\u2019ll\nencounter:\nPerceptron\nA perceptron is the simplest type of neural network \u2014 just one neuron. It takes\ninputs, multiplies them by weights, adds a bias, and applies an activation function\nto make a decision (e.g., classify 0 or 1).\nNeural Network\nA neural network is a collection of interconnected layers of perceptrons (neurons).\nEach layer transforms its inputs using weights, biases, and activation functions.\nDeep neural networks have multiple hidden layers and can model complex\npatterns.\nHyperparameters\nThese are settings we configure before training a model. They are not learned\nfrom the data. Examples include:\nLearning rate: How much to adjust weights during training\nNumber of epochs: How many times the model sees the entire training\ndataset\nBatch size: How many samples to process before updating weights\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 9, "text": "Number of layers or neurons: How many neurons are in each layer of the\nnetwork\nLearning Rate (\u03b7)\nThis controls how much we adjust the weights after each training step. A learning\nrate that\u2019s too high may overshoot the solution; too low may make training very\nslow.\nTraining\nThis is the process where the model learns patterns from data by updating\nweights based on errors between predicted and actual outputs.\nBackpropagation\nBackpropagation is the algorithm used to update weights in a neural network. It\ncalculates the gradient of the loss function with respect to each weight by applying\nthe chain rule, allowing the model to learn from its mistakes.\nInference\nInference is when the trained model is used to make predictions on new, unseen\ndata.\n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 10, "text": "Activation Function\nThis function adds non-linearity to the output of neurons, helping networks model\ncomplex patterns.\nCommon activation functions:\nReLU : Rectified Linear Unit\nSigmoid : squashes output between 0 and 1\nTanh : squashes output between -1 and 1\nPerceptrons typically use a step function as the activation, but modern neural\nnets often use ReLU .\nEpoch\nOne epoch means one full pass over the entire training dataset. Multiple epochs\nare used so the model can keep refining its understanding.\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 11, "text": "Training a Perceptron with scikit-learn\n1. Import Libraries\n2. Create and Split Data\n3. Initialize the Perceptron\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\nrandom_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf = Perceptron(\n    max_iter=1000,\n# Maximum number of epochs\n    eta0=0.1,\n# Learning rate\n    random_state=42,\n# For reproducibility\n    tol=1e-3,\n# Stop early if improvement is smaller than this\n    shuffle=True\n# Shuffle data each epoch\n)\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 12, "text": "4. Train the Model\nUnder the hood, this performs the following steps:\nLoops through the data up to max_iter  times (epochs)\nComputes predictions\nIf a prediction is wrong, updates weights\n5. Evaluate the Model\nImportant Hyperparameters Recap:\nHyperparameter\nDescription\nmax_iter\nNumber of epochs (passes over training data)\neta0\nLearning rate\ntol\nTolerance for stopping early\nshuffle\nWhether to shuffle data between epochs\nrandom_state\nSeed for reproducibility\nclf.fit(X_train, y_train)\n\u2022 \n\u2022 \n\u2022 \naccuracy = clf.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 13, "text": "TensorFlow vs Keras vs PyTorch\nDeep learning has transformed industries\u2014from self-driving cars to language\nmodels like ChatGPT. But behind the scenes, there are powerful libraries that make\nall of this possible: TensorFlow, Keras, and PyTorch. Today, we\u2019ll explore:\nWhy and how each library was created\nHow they differ in philosophy and design\nWhat a \u201cbackend\u201d means in Keras\nWhich one might be right for you\nA Brief History\n1. TensorFlow\nReleased: 2015 by Google Brain\nLanguage: Python (but has C++ core)\nGoal: Provide an efficient, production-ready, and scalable library for deep\nlearning.\nTensorFlow is a computational graph framework, meaning it represents\ncomputations as nodes in a graph.\nOpen sourced in 2015, TensorFlow quickly became the go-to library for many\ncompanies and researchers.\nFun Fact: TensorFlow was a spiritual successor to Google\u2019s earlier tool called \nDistBelief.\n2. Keras\nReleased: 2015 by Fran\u00e7ois Chollet\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 14, "text": "Goal: Make deep learning simple, intuitive, and user-friendly.\nKeras was originally just a high-level wrapper over Theano and TensorFlow,\nmaking it easier to build models with fewer lines of code.\nKeras introduced the idea of writing deep learning models like stacking Lego\nblocks.\nKeras was not a full deep learning engine\u2014it needed a \u201cbackend\u201d to actually do\nthe math\n3. PyTorch\nReleased: 2016 by Facebook AI Research (FAIR)\nLanguage: Python-first, with a strong integration to NumPy\nGoal: Make deep learning flexible, dynamic, and easier for research.\nPyTorch uses dynamic computation graphs, meaning the graph is built on the\nfly, allowing for more intuitive debugging and flexibility.\nPyTorch gained massive popularity in academia and research because of its\nPythonic nature and simplicity.\nIn a nut shell\u2026\nFeature\nTensorFlow\nKeras\nPyTorch\nDeveloped\nBy\nGoogle\nFran\u00e7ois\nChollet\n(Google)\nFacebook (Meta)\nLevel\nLow-level & high-\nlevel\nHigh-level\nonly\nLow-level (with\nsome high-level\nAPIs)\nComputation\nGraph\nStatic (TensorFlow\n1.x), Hybrid (2.x)\nDepends on\nbackend\nDynamic\nEase of Use\nVery high\nHigh\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 15, "text": "Feature\nTensorFlow\nKeras\nPyTorch\nMedium (Better in\n2.x)\nDebugging\nHarder (in 1.x),\nEasier in 2.x\nEasy\nVery easy (Pythonic)\nProduction-\nready\nYes\nYes (via\nTensorFlow\nbackend)\nGaining ground\nResearch\nusage\nModerate\nModerate\nVery high\nWhat Is a \u201cBackend\u201d in Keras?\nKeras itself doesn\u2019t perform computations like matrix multiplications or gradient\ndescent. It\u2019s more like a user interface or a frontend. It delegates the heavy lifting\nto a backend engine.\nSupported Backends Over Time:\nTheano (now discontinued)\nTensorFlow (default backend now)\nCNTK (Microsoft, also discontinued)\nPlaidML (experimental support)\nYou can think of Keras as the steering wheel, while TensorFlow or Theano was the \nengine under the hood.\nIn TensorFlow 2.0 and later, Keras is fully integrated as tf.keras , eliminating the\nneed to manage separate backends.\n\u2022 \n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 16, "text": "TensorFlow vs PyTorch: The Real Battle\nArea\nTensorFlow\nPyTorch\nEase of\nDeployment\nTensorFlow Serving, TFX,\nTensorFlow Lite\nTorchServe, ONNX, some\ncatching up\nMobile\nSupport\nExcellent (TF Lite, TF.js)\nImproving but limited\nDynamic\nGraphs\nTF 1.x: No, TF 2.x: Yes (via\nAutograph)\nNative support\nCommunity\nLarge, especially in production\nMassive in academia\nPerformance\nHighly optimized\nAlso strong, especially for\nGPUs\nWhich One Should You Learn First?\nBeginner? \u2192 Start with Keras via TensorFlow 2.x ( tf.keras ). It\u2019s simple and\nproduction-ready.\nResearcher or experimenting a lot? \u2192 Use PyTorch.\nLooking for deployment & scalability? \u2192 TensorFlow is very robust.\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 17, "text": "Installing TensorFlow 2.0\nToday we will install TensorFlow 2.0, which is a powerful library for machine\nlearning and deep learning tasks. TensorFlow 2.0 simplifies the process of building\nand training models, making it more user-friendly compared to its predecessor.\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 18, "text": "Understanding and Visualizing the\nMNIST Dataset with TensorFlow\nThe MNIST dataset is like the \u201cHello World\u201d of deep learning. It contains 70,000\ngrayscale images of handwritten digits (0\u20139), each of size 28x28 pixels. Before we\ntrain a neural network, it\u2019s important to understand what we\u2019re working with.\nToday we\u2019ll:\nLoad the MNIST dataset using TensorFlow\nVisualize a few digit samples\nUnderstand the data format\nStep 1: Load the Dataset\nTensorFlow provides a built-in method to load MNIST, so no extra setup is needed.\nOutput:\n\u2022 \n\u2022 \n\u2022 \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n# Load dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n# Check the shape\nprint(\"Training data shape:\", x_train.shape)\nprint(\"Training labels shape:\", y_train.shape)\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 19, "text": "We have 60,000 training images and 10,000 test images.\nEach image is 28x28 pixels.\nEach label is a number from 0 to 9.\nStep 2: Visualize Sample Digits\nLet\u2019s look at a few images to get a feel for the dataset:\nThis will display the first 10 handwritten digits with their corresponding labels\nabove them.\nWhat You Should Notice\nThe digits vary in writing style, which makes this dataset great for teaching\ncomputers to generalize.\nAll images are normalized 28x28 grayscale\u2014no color channels.\nThe label is not embedded in the image; it\u2019s provided separately.\nTraining data shape: (60000, 28, 28)\nTraining labels shape: (60000,)\n\u2022 \n\u2022 \n\u2022 \n# Plot first 10 images with their labels\nplt.figure(figsize=(10, 2))\nfor i in range(10):\n    plt.subplot(1, 10, i + 1)\n    plt.imshow(x_train[i], cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(str(y_train[i]))\nplt.tight_layout()\nplt.show()\n\u2022 \n\u2022 \n\u2022 \n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 20, "text": "Your First Neural Network\nTensorFlow is an open-source deep learning library developed by Google. It\u2019s\nwidely used in industry and academia for building and training machine learning\nmodels. TensorFlow 2.0 brought significant improvements in ease of use, especially\nwith eager execution and tight integration with Keras.\nIn this tutorial, we\u2019ll create a simple neural network that learns to classify\nhandwritten digits using the MNIST dataset. This dataset contains 28x28 grayscale\nimages of digits from 0 to 9.\nKey Features of TensorFlow 2.0\nEager execution by default (no more complex session graphs!)\nKeras as the official high-level API ( tf.keras )\nBetter debugging and simplicity\nGreat for both beginners and professionals\nWhat is a Neural Network?\nA neural network is a collection of layers that learn to map input data to outputs.\nThink of layers as filters that extract meaningful patterns. Each layer applies\ntransformations using weights and activation functions.\nInstallation\n\u2022 \n\u2022 \n\u2022 \n\u2022 \npip install tensorflow\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 21, "text": "Importing Required Libraries\nLoading and Preparing the Data\nBuilding a Simple Neural Network\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\n# Load the data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n# Normalize the input data\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n# One-hot encode the labels\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n# 28x28 images to 784 input features\n    Dense(128, activation='relu'),\n# Hidden layer with 128 neurons\n    Dense(10, activation='softmax')\n# Output layer for 10 classes\n])\n"}, {"number": "14", "title": "Introduction to Neural Networks", "page_no": 22, "text": "Compiling the Model\nTraining the Model\nEvaluating the Model\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n"}]